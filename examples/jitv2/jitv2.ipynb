{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1ca505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tilelang.language.v2 as tl\n",
    "import torch\n",
    "from tilelang import PassConfigKey\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af9e28",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fe3e8",
   "metadata": {},
   "source": [
    "### Simple GEMM Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49622aa1",
   "metadata": {},
   "source": [
    "\n",
    "Tilelang jit v2 allow you to write `tl.Tensor` to represent a schema for torch.Tensor\n",
    "* `tl.Tensor[int]` is an 1-dimensional tensor, while `tl.Tensor[int, int]` is a 2-dimensional tensor\n",
    "\n",
    "You can allocate global buffer using `tl.empty`, and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ef5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tl.jit\n",
    "def gemm(\n",
    "    A: tl.Tensor[int, int],\n",
    "    B: tl.Tensor[int, int],\n",
    "    accum_dtype: torch.dtype = torch.float32,\n",
    "    block_N: int = 128,\n",
    "    block_M: int = 128,\n",
    "    block_K: int = 128,\n",
    "):\n",
    "    # use A.xxx_params() to get params\n",
    "    #  params binding checks all param are the same\n",
    "    N, K = A.shape_params()\n",
    "    M, K = B.shape_params()\n",
    "\n",
    "    C = tl.empty((M, N), dtype=accum_dtype)\n",
    "    dims = [\n",
    "        tl.ceildiv(M, block_M),\n",
    "        tl.ceildiv(N, block_N),\n",
    "    ]\n",
    "    with tl.Kernel(*dims, threads=128) as (bx, by):\n",
    "        A_shared = tl.alloc_shared((block_M, block_K), dtype=A.dtype)\n",
    "        B_shared = tl.alloc_shared((block_K, block_N), dtype=B.dtype)\n",
    "        C_local = tl.alloc_fragment((block_M, block_N), dtype=accum_dtype)\n",
    "        tl.clear(C_local)\n",
    "        for k in tl.Pipelined(tl.ceildiv(K, block_K), num_stages=3):\n",
    "            tl.copy(A[by * block_M, k * block_K], A_shared)\n",
    "            tl.copy(B[k * block_K, bx * block_N], B_shared)\n",
    "            tl.gemm(A_shared, B_shared, C_local)\n",
    "        tl.copy(C_local, C[by * block_M, bx * block_N])\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b3756",
   "metadata": {},
   "source": [
    "Call the kernel with torch tensor will trigger jit compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d7b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1024, 1024, dtype=torch.float16).cuda()\n",
    "B = torch.randn(1024, 1024, dtype=torch.float16).cuda()\n",
    "C_tl = gemm(A, B)\n",
    "C_torch = A.float() @ B.float()\n",
    "torch.testing.assert_close(C_torch, C_tl, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fa333",
   "metadata": {},
   "source": [
    "Benchmarking the kernel by `do_bench`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c9f05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012575408695652115"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tilelang.profiler import do_bench\n",
    "do_bench(lambda: gemm(A, B))\n",
    "# or you can use gemm.bench(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05c493",
   "metadata": {},
   "source": [
    "### Pointer Based Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a29ecd",
   "metadata": {},
   "source": [
    "JITv2 allow you to use pointer and make_tensor to write kernel, this is very similar to triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02eafddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tl.jit\n",
    "def gemm_ptr(\n",
    "    A_ptr: tl.ptr,\n",
    "    B_ptr: tl.ptr,\n",
    "    M: int,\n",
    "    N: int,\n",
    "    K: int,\n",
    "    dtype: torch.dtype = torch.float16,\n",
    "    accum_dtype: torch.dtype = torch.float32,\n",
    "    block_N: int = 128,\n",
    "    block_M: int = 128,\n",
    "    block_K: int = 128,\n",
    "):\n",
    "    # `tl.empty` is before `tl.make_tensor`, because `tl.empty` is host code, and `tl.make_tensor` is device code\n",
    "    C = tl.empty((M, N), dtype=accum_dtype)\n",
    "\n",
    "    A = tl.make_tensor(A_ptr, (M, K), dtype=dtype)\n",
    "    B = tl.make_tensor(B_ptr, (N, K), dtype=dtype)\n",
    "    dims = [\n",
    "        tl.ceildiv(M, block_M),\n",
    "        tl.ceildiv(N, block_N),\n",
    "    ]\n",
    "    with tl.Kernel(*dims, threads=128) as (bx, by):\n",
    "        A_shared = tl.alloc_shared((block_M, block_K), dtype=A.dtype)\n",
    "        B_shared = tl.alloc_shared((block_K, block_N), dtype=B.dtype)\n",
    "        C_local = tl.alloc_fragment((block_M, block_N), dtype=accum_dtype)\n",
    "        tl.clear(C_local)\n",
    "        for k in tl.Pipelined(tl.ceildiv(K, block_K), num_stages=3):\n",
    "            tl.copy(A[by * block_M, k * block_K], A_shared)\n",
    "            tl.copy(B[k * block_K, bx * block_N], B_shared)\n",
    "            tl.gemm(A_shared, B_shared, C_local)\n",
    "        tl.copy(C_local, C[by * block_M, bx * block_N])\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc4020",
   "metadata": {},
   "source": [
    "You need to manually convert a tensor to its pointer to call the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc79a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1024, 1024, dtype=torch.float16).cuda()\n",
    "B = torch.randn(1024, 1024, dtype=torch.float16).cuda()\n",
    "C_torch = A.float() @ B.float()\n",
    "C_tl = gemm_ptr(A.data_ptr(), B.data_ptr(), 1024, 1024, 1024, dtype=torch.float16)\n",
    "torch.testing.assert_close(C_torch, C_tl, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dff841",
   "metadata": {},
   "source": [
    "### Dynamic Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fdeca",
   "metadata": {},
   "source": [
    "Use `tl.dyn` to define dynamic argument, dynamic argument is a Var in TVM.\n",
    "\n",
    "* `tl.dyn` is a dynamic int\n",
    "* `tl.dyn[float]` is a dynamic float\n",
    "* `tl.dyn[int, '_N']` is a dynamic int with name \"_N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88bab489",
   "metadata": {},
   "outputs": [],
   "source": [
    "_N = tl.dyn[int, '_N']\n",
    "@tl.jit\n",
    "def vec_add(\n",
    "    A: tl.Tensor[_N],\n",
    "    B: tl.Tensor[_N],\n",
    "    block_N: int = 128 * 8,\n",
    "):\n",
    "    N, = A.shape\n",
    "    assert A.dtype == B.dtype, \"Expect 2 tensor with the same dtype\"\n",
    "    C = tl.empty((N,), dtype=A.dtype)\n",
    "    with tl.Kernel(tl.ceildiv(N, block_N), threads=128) as bx:\n",
    "        px = bx * block_N\n",
    "        for i in tl.Parallel(block_N):\n",
    "            C[px + i] = A[px + i] + B[px + i]\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa97a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(2**20, dtype=torch.float16).cuda()\n",
    "B = torch.randn(2**20, dtype=torch.float16).cuda()\n",
    "C_torch = A + B\n",
    "C_tl = vec_add(A, B)\n",
    "torch.testing.assert_close(C_torch, C_tl, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93e6c7",
   "metadata": {},
   "source": [
    "Unnamed dynamic argument is also supported, you can use it both in tensor and as scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "201fbd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tl.jit\n",
    "def vec_add_scalar(\n",
    "    A: tl.Tensor[int],\n",
    "    cval: tl.dyn[float],\n",
    "    block_N: int = 128 * 8\n",
    "):\n",
    "    N, = A.shape\n",
    "    assert A.dtype == tl.get_tvm_dtype(float), \"Expect A to have float dtype\"\n",
    "    C = tl.empty((N,), dtype=A.dtype)\n",
    "    with tl.Kernel(tl.ceildiv(N, block_N), threads=128) as bx:\n",
    "        px = bx * block_N\n",
    "        for i in tl.Parallel(block_N):\n",
    "            C[px + i] = A[px + i] + cval\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa3c3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(2**20, dtype=torch.float32).cuda()\n",
    "cval = 1.0\n",
    "res_torch = A + cval\n",
    "res_tl = vec_add_scalar(A, cval)\n",
    "torch.testing.assert_close(res_torch, res_tl, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb58957",
   "metadata": {},
   "source": [
    "### Strided Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cda842",
   "metadata": {},
   "source": [
    "use tl.StridedTensor to to use strided tensor, you need to annotate which is static and which is dynamic\n",
    "* Type hinting is not implemented in strided tensor, sorry..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77aaca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tl.jit\n",
    "def get_contingous(\n",
    "    # Fixed Shape, Variable Stride\n",
    "    A: tl.StridedTensor[(int, int), (tl.dyn, tl.dyn)]\n",
    "):\n",
    "    M, N, dtype = A.params()\n",
    "    C = tl.empty((M, N), dtype=dtype)\n",
    "    with tl.Kernel(tl.ceildiv(M, 128), tl.ceildiv(N, 128), threads=128) as (bx, by):\n",
    "        tl.copy(\n",
    "            C[bx * 128: (bx + 1) * 128, by * 128: (by + 1) * 128],\n",
    "            A[bx * 128: (bx + 1) * 128, by * 128: (by + 1) * 128],\n",
    "        )\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddd98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1024, 4, 1024, 4, dtype=torch.float16).cuda()\n",
    "out = get_contingous(A[:, 0, :, 0])\n",
    "gold = A[:, 0, :, 0].clone()\n",
    "torch.testing.assert_close(out, gold, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da916fb6",
   "metadata": {},
   "source": [
    "## Call Overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f633fb6",
   "metadata": {},
   "source": [
    "JITv2 has extremely low runtime overhead\n",
    "* about **3us** additional overhead compared to torch native kernel\n",
    "* the main overhead comes from\n",
    "    * parse argument: ~1us\n",
    "    * torch.current_stream: ~0.2us\n",
    "    * torch.device('cuda'): ~0.2us\n",
    "    * torch.empty: 1~2us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "468ed050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMM 1024 x 1024 x 1024\n",
      "Torch time:     19.696090300567448 us\n",
      "Tilelang time:  12.795752682723105 us\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print('GEMM 1024 x 1024 x 1024')\n",
    "\n",
    "A = torch.randn(1024, 1024, dtype=torch.float16).cuda()\n",
    "B = torch.randn(1024, 1024, dtype=torch.float16).cuda()\n",
    "torch_beg = time.perf_counter()\n",
    "for _ in range(10000):\n",
    "    A @ B\n",
    "torch_end = time.perf_counter()\n",
    "elapsed = (torch_end - torch_beg) / 10000 * 1e6\n",
    "\n",
    "print('Torch time:    ', elapsed, 'us')\n",
    "\n",
    "tl_beg = time.perf_counter()\n",
    "for _ in range(10000):\n",
    "    gemm(A, B)\n",
    "tl_end = time.perf_counter()\n",
    "elapsed = (tl_end - tl_beg) / 10000 * 1e6\n",
    "\n",
    "print('Tilelang time: ', elapsed, 'us')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "478d35cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vec Add 1024 x 1024\n",
      "Torch time:     7.411740510724485 us\n",
      "Tilelang time:  9.731462597846985 us\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print('Vec Add 1024 x 1024')\n",
    "\n",
    "A = torch.randn(1024 * 1024, dtype=torch.float16).cuda()\n",
    "B = torch.randn(1024 * 1024, dtype=torch.float16).cuda()\n",
    "torch_beg = time.perf_counter()\n",
    "for _ in range(10000):\n",
    "    A + B\n",
    "torch_end = time.perf_counter()\n",
    "elapsed = (torch_end - torch_beg) / 10000 * 1e6\n",
    "\n",
    "print('Torch time:    ', elapsed, 'us')\n",
    "\n",
    "tl_beg = time.perf_counter()\n",
    "for _ in range(10000):\n",
    "    vec_add(A, B)\n",
    "tl_end = time.perf_counter()\n",
    "elapsed = (tl_end - tl_beg) / 10000 * 1e6\n",
    "\n",
    "print('Tilelang time: ', elapsed, 'us')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb8473",
   "metadata": {},
   "source": [
    "## Autotune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceede9ec",
   "metadata": {},
   "source": [
    "**We highly suggest to use autotune in development and make the parameter fixed in deployment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80bbcc6",
   "metadata": {},
   "source": [
    "### Run autotune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b276b3e",
   "metadata": {},
   "source": [
    "Run autotune with `ker.tune`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f69c792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:42:01  [TileLang:tilelang.language.v2.jit:INFO]: Elaborate 18 configs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46eac7e0b94f4a96a583bad9b120f6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Elaborating:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc1dff13fa544389ef6a148270ca9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parallel Compiling:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1159989eee42e1ba0c77250a675636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AutoTuneResult(\n",
       "  name=gemm,\n",
       "  num_errors=0,\n",
       "  best_latency=0.01013209827044034,\n",
       "  best_args=gemm(\n",
       "    tl.place(1024, 1024, dtype=torch.float16),\n",
       "    tl.place(1024, 1024, dtype=torch.float16),\n",
       "    block_M=64,\n",
       "    block_N=128,\n",
       "    block_K=64\n",
       "  ),\n",
       "  best={'A.dtype': torch.float16, 'A__shape_0': 1024, 'A__shape_1': 1024, 'A__stride_0': 1024, 'A__stride_1': 1, 'B.dtype': torch.float16, 'B__shape_0': 1024, 'B__shape_1': 1024, 'B__stride_0': 1024, 'B__stride_1': 1, 'accum_dtype': torch.float32, 'block_N': 128, 'block_M': 64, 'block_K': 64, 'latency': 0.01013209827044034, '_status': 'Success', '_error': ''},\n",
       "  records=<18 records>,\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(1024, 1024, dtype=torch.float16).cuda()\n",
    "B = torch.randn(1024, 1024, dtype=torch.float16).cuda()\n",
    "tune_result = gemm.tune(\n",
    "    A,\n",
    "    B,\n",
    "    block_M=tl.tune([64, 128, 256]),\n",
    "    block_N=tl.tune([64, 128, 256]),\n",
    "    block_K=tl.tune([32, 64]),\n",
    ")\n",
    "tune_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "144f9eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:42:13  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.010162922123893914"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_kwargs = tune_result.best_args.kwargs\n",
    "gemm.bench(A, B, **best_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5447b",
   "metadata": {},
   "source": [
    "The tune result can be converted into pandas for futher analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c15b454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_52349_row3_col0, #T_52349_row3_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_52349\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"index_name level0\" >block_K</th>\n",
       "      <th id=\"T_52349_level0_col0\" class=\"col_heading level0 col0\" >32</th>\n",
       "      <th id=\"T_52349_level0_col1\" class=\"col_heading level0 col1\" >64</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >block_N</th>\n",
       "      <th class=\"index_name level1\" >block_M</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_52349_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"3\">64</th>\n",
       "      <th id=\"T_52349_level1_row0\" class=\"row_heading level1 row0\" >64</th>\n",
       "      <td id=\"T_52349_row0_col0\" class=\"data row0 col0\" >0.023410</td>\n",
       "      <td id=\"T_52349_row0_col1\" class=\"data row0 col1\" >0.015334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52349_level1_row1\" class=\"row_heading level1 row1\" >128</th>\n",
       "      <td id=\"T_52349_row1_col0\" class=\"data row1 col0\" >0.014278</td>\n",
       "      <td id=\"T_52349_row1_col1\" class=\"data row1 col1\" >0.010503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52349_level1_row2\" class=\"row_heading level1 row2\" >256</th>\n",
       "      <td id=\"T_52349_row2_col0\" class=\"data row2 col0\" >0.017084</td>\n",
       "      <td id=\"T_52349_row2_col1\" class=\"data row2 col1\" >0.013397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52349_level0_row3\" class=\"row_heading level0 row3\" rowspan=\"3\">128</th>\n",
       "      <th id=\"T_52349_level1_row3\" class=\"row_heading level1 row3\" >64</th>\n",
       "      <td id=\"T_52349_row3_col0\" class=\"data row3 col0\" >0.013945</td>\n",
       "      <td id=\"T_52349_row3_col1\" class=\"data row3 col1\" >0.010132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52349_level1_row4\" class=\"row_heading level1 row4\" >128</th>\n",
       "      <td id=\"T_52349_row4_col0\" class=\"data row4 col0\" >0.016207</td>\n",
       "      <td id=\"T_52349_row4_col1\" class=\"data row4 col1\" >0.013270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52349_level1_row5\" class=\"row_heading level1 row5\" >256</th>\n",
       "      <td id=\"T_52349_row5_col0\" class=\"data row5 col0\" >0.136026</td>\n",
       "      <td id=\"T_52349_row5_col1\" class=\"data row5 col1\" >0.143063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52349_level0_row6\" class=\"row_heading level0 row6\" rowspan=\"3\">256</th>\n",
       "      <th id=\"T_52349_level1_row6\" class=\"row_heading level1 row6\" >64</th>\n",
       "      <td id=\"T_52349_row6_col0\" class=\"data row6 col0\" >0.016845</td>\n",
       "      <td id=\"T_52349_row6_col1\" class=\"data row6 col1\" >0.012954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52349_level1_row7\" class=\"row_heading level1 row7\" >128</th>\n",
       "      <td id=\"T_52349_row7_col0\" class=\"data row7 col0\" >0.195256</td>\n",
       "      <td id=\"T_52349_row7_col1\" class=\"data row7 col1\" >0.213166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52349_level1_row8\" class=\"row_heading level1 row8\" >256</th>\n",
       "      <td id=\"T_52349_row8_col0\" class=\"data row8 col0\" >0.746829</td>\n",
       "      <td id=\"T_52349_row8_col1\" class=\"data row8 col1\" >0.735000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6293701310>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    tune_result\n",
    "    .to_pandas()\n",
    "    .pivot_table(index=['block_N', 'block_M'], columns='block_K', values='latency')\n",
    "    .style\n",
    "    .highlight_min(color='lightgreen')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597497e7",
   "metadata": {},
   "source": [
    "### Advanced Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00392dc",
   "metadata": {},
   "source": [
    "Autotune is divided into 2 steps:\n",
    "1. `kerl.get_tune_configs` to obtain all configs for tunning\n",
    "2. `kerl.tune_configs` to tune all the configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56df419e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CallArgs(\n",
       "   tl.place(1024, 1024, dtype=torch.float16),\n",
       "   tl.place(1024, 1024, dtype=torch.float16),\n",
       "   block_M=64,\n",
       "   block_N=64,\n",
       "   block_K=32\n",
       " ),\n",
       " CallArgs(\n",
       "   tl.place(1024, 1024, dtype=torch.float16),\n",
       "   tl.place(1024, 1024, dtype=torch.float16),\n",
       "   block_M=64,\n",
       "   block_N=64,\n",
       "   block_K=64\n",
       " )]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemm.get_tune_configs(\n",
    "    A,\n",
    "    B,\n",
    "    block_M=tl.tune([64, 128, 256]),\n",
    "    block_N=tl.tune([64, 128, 256]),\n",
    "    block_K=tl.tune([32, 64]),\n",
    ")[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c512611",
   "metadata": {},
   "source": [
    "For your advanced tuning, you can write a generator to generate call args:\n",
    "* Return a **dict** to call the function with `**kwargs`\n",
    "* Return a **list/tuple** to call the function with `*args`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5fbd834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:42:52  [TileLang:tilelang.language.v2.jit:INFO]: Elaborate 12 configs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711c62011b234bbebd0814ddbd19b1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Elaborating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:42:52  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:42:52  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:42:52  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:42:52  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:42:52  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm` with `out_idx=[2]`\n",
      "2025-10-13 15:42:52  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:42:52  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:42:52  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:42:52  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:42:52  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:42:52  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:42:52  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960c94edea67493b86c58b3bf9ffe4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parallel Compiling:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:43:09  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb35184b8c748e2ab895e58bf7194be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AutoTuneResult(\n",
       "  name=gemm,\n",
       "  num_errors=0,\n",
       "  best_latency=0.009626281249999693,\n",
       "  best_args=gemm(\n",
       "    A=tl.place(1024, 1024, dtype=torch.float16),\n",
       "    B=tl.place(1024, 1024, dtype=torch.float16),\n",
       "    block_N=128,\n",
       "    block_M=64,\n",
       "    block_K=128\n",
       "  ),\n",
       "  best={'A.dtype': torch.float16, 'A__shape_0': 1024, 'A__shape_1': 1024, 'A__stride_0': 1024, 'A__stride_1': 1, 'B.dtype': torch.float16, 'B__shape_0': 1024, 'B__shape_1': 1024, 'B__stride_0': 1024, 'B__stride_1': 1, 'accum_dtype': torch.float32, 'block_N': 128, 'block_M': 64, 'block_K': 128, 'latency': 0.009626281249999693, '_status': 'Success', '_error': ''},\n",
       "  records=<12 records>,\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "def generate_tune_args(A, B):\n",
    "    for n, m, k in product((64, 128), (32, 64), (32, 64, 128)):\n",
    "        yield {\n",
    "            'A': A,\n",
    "            'B': B,\n",
    "            'block_N': n,\n",
    "            'block_M': m,\n",
    "            'block_K': k\n",
    "        }\n",
    "gemm.tune_configs(generate_tune_args(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43985e49",
   "metadata": {},
   "source": [
    "### Tune Pass Configs and Compilation Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f086bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tilelang import PassConfigKey\n",
    "@tl.jit\n",
    "def gemm_tune_advanced(\n",
    "    A: tl.Tensor[int, int],\n",
    "    B: tl.Tensor[int, int],\n",
    "    accum_dtype: torch.dtype = torch.float32,\n",
    "    block_N: int = 128,\n",
    "    block_M: int = 128,\n",
    "    block_K: int = 128,\n",
    "    disable_tma: bool = False,\n",
    "    use_prec_sqrt: bool = False,\n",
    "):\n",
    "    tl.set_pass_configs({\n",
    "        PassConfigKey.TL_DISABLE_TMA_LOWER: disable_tma\n",
    "    })\n",
    "    if use_prec_sqrt:\n",
    "        tl.add_compile_flags(['--prec-sqrt', 'true'])\n",
    "    (M, K), (N, K2) = A.shape, B.shape\n",
    "    assert K == K2, \"Expect matrix A and B to have the same number of columns\"\n",
    "    C = tl.empty((M, N), dtype=accum_dtype)\n",
    "    dims = [\n",
    "        tl.ceildiv(M, block_M),\n",
    "        tl.ceildiv(N, block_N),\n",
    "    ]\n",
    "    with tl.Kernel(*dims, threads=128) as (bx, by):\n",
    "        A_shared = tl.alloc_shared((block_M, block_K), dtype=A.dtype)\n",
    "        B_shared = tl.alloc_shared((block_K, block_N), dtype=B.dtype)\n",
    "        C_local = tl.alloc_fragment((block_M, block_N), dtype=accum_dtype)\n",
    "        tl.clear(C_local)\n",
    "        for k in tl.Pipelined(tl.ceildiv(K, block_K), num_stages=3):\n",
    "            tl.copy(A[by * block_M, k * block_K], A_shared)\n",
    "            tl.copy(B[k * block_K, bx * block_N], B_shared)\n",
    "            for i, j in tl.Parallel(block_M, block_K):\n",
    "                A_shared[i, j] = tl.sqrt(A_shared[i, j])\n",
    "            tl.gemm(A_shared, B_shared, C_local)\n",
    "        tl.copy(C_local, C[by * block_M, bx * block_N])\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6fa139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:43:50  [TileLang:tilelang.language.v2.jit:INFO]: Elaborate 4 configs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed52890324aa49f1ba1e289270d52a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Elaborating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:43:50  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm_tune_advanced` with `out_idx=[2]`\n",
      "2025-10-13 15:43:50  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm_tune_advanced` with `out_idx=[2]`\n",
      "2025-10-13 15:43:50  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm_tune_advanced` with `out_idx=[2]`\n",
      "2025-10-13 15:43:50  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm_tune_advanced` with `out_idx=[2]`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37599bb72b544b40a607a84b5726fcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parallel Compiling:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:44:02  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm_tune_advanced`\n",
      "2025-10-13 15:44:02  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm_tune_advanced`\n",
      "2025-10-13 15:44:02  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm_tune_advanced`\n",
      "2025-10-13 15:44:02  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm_tune_advanced`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3c332df6e149b3a1923512402a0a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>use_prec_sqrt</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disable_tma</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>0.150496</td>\n",
       "      <td>0.150485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.150605</td>\n",
       "      <td>0.150277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "use_prec_sqrt     False     True \n",
       "disable_tma                      \n",
       "False          0.150496  0.150485\n",
       "True           0.150605  0.150277"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(1024, 1024, dtype=torch.float16).cuda()\n",
    "B = torch.randn(1024, 1024, dtype=torch.float16).cuda()\n",
    "result = gemm_tune_advanced.tune(\n",
    "    A, B,\n",
    "    disable_tma=tl.tune([True, False]),\n",
    "    use_prec_sqrt=tl.tune([True, False])\n",
    ")\n",
    "result.to_pandas().pivot_table(index='disable_tma', columns='use_prec_sqrt', values='latency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2dfd55",
   "metadata": {},
   "source": [
    "## Other Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6942263",
   "metadata": {},
   "source": [
    "### Parallel Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae5eca",
   "metadata": {},
   "source": [
    "Use ker.par_compile to compile many kernel args parallely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8acea014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:44:06  [TileLang:tilelang.language.v2.jit:INFO]: Elaborate 5 configs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6541cc53da41bf82d580afc254ec6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Elaborating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:44:06  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm` with `out_idx=[2]`\n",
      "2025-10-13 15:44:06  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm` with `out_idx=[2]`\n",
      "2025-10-13 15:44:06  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:44:06  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm` with `out_idx=[2]`\n",
      "2025-10-13 15:44:06  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm` with `out_idx=[2]`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd39b740a72d478f82d2beb8d125800a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parallel Compiling:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:44:17  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm`\n",
      "2025-10-13 15:44:17  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm`\n",
      "2025-10-13 15:44:17  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm`\n",
      "2025-10-13 15:44:17  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm`\n"
     ]
    }
   ],
   "source": [
    "def generate_args():\n",
    "    available_nmk = [\n",
    "        (1024, 2048, 1024),\n",
    "        (1024, 1024, 1024),\n",
    "        (2048, 1024, 1024),\n",
    "        (1024, 2048, 2048),\n",
    "        (2048, 2048, 2048),\n",
    "    ]\n",
    "    for n, m, k in available_nmk:\n",
    "        yield {\n",
    "            'A': tl.place(n, k, dtype=torch.float16),\n",
    "            'B': tl.place(m, k, dtype=torch.float16),\n",
    "        }\n",
    "_ = gemm.par_compile(generate_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdfee63",
   "metadata": {},
   "source": [
    "Use `tl.empty_data_ptr()` as a place holder for `tl.ptr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cdab01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:44:29  [TileLang:tilelang.language.v2.jit:INFO]: Elaborate 5 configs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924104ac91494342b1001af41f75a5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Elaborating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:44:29  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm_ptr` with `out_idx=[0]`\n",
      "2025-10-13 15:44:29  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n",
      "2025-10-13 15:44:29  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm_ptr` with `out_idx=[0]`\n",
      "2025-10-13 15:44:29  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm_ptr` with `out_idx=[0]`\n",
      "2025-10-13 15:44:29  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `gemm_ptr` with `out_idx=[0]`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495fbeed0e51413e8afbcf838f1c4a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parallel Compiling:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:44:40  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm_ptr`\n",
      "2025-10-13 15:44:40  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm_ptr`\n",
      "2025-10-13 15:44:40  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm_ptr`\n",
      "2025-10-13 15:44:40  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `gemm_ptr`\n"
     ]
    }
   ],
   "source": [
    "def generate_args_ptr():\n",
    "    available_nmk = [\n",
    "        (1024, 2048, 1024),\n",
    "        (1024, 1024, 1024),\n",
    "        (2048, 1024, 1024),\n",
    "        (1024, 2048, 2048),\n",
    "        (2048, 2048, 2048),\n",
    "    ]\n",
    "    for n, m, k in available_nmk:\n",
    "        yield {\n",
    "            'A_ptr': tl.empty_data_ptr(),\n",
    "            'B_ptr': tl.empty_data_ptr(),\n",
    "            'M': n,\n",
    "            'N': m,\n",
    "            'K': k,\n",
    "        }\n",
    "_ = gemm_ptr.par_compile(generate_args_ptr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2b3fd",
   "metadata": {},
   "source": [
    "## JITv2 Internals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd09469",
   "metadata": {},
   "source": [
    "JIT Compilation Flow:\n",
    "1. User write Tilelang Kernel\n",
    "2. **Elaboration**: Tilelang Kernel is converted to JITFunc\n",
    "    * JITFunc contains all required data for compilation\n",
    "3. **Compilation**: JITFunc is compiled to JITKernel\n",
    "    * JITKernel is callable\n",
    "\n",
    "You can use `kernel.partial` to generate the JITFunc, and manually compile it\n",
    "* You can give it `torch.Tensor` or just `tl.place` as placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1469de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JITFunc(\n",
       "  target=cuda -keys=cuda,gpu -arch=sm_90 -max_num_threads=1024 -thread_warp_size=32,\n",
       "  target_host=None,\n",
       "  global_allocs=[BufferSchema(name='C', shape=[1024, 1024], stride=[1024, 1], dtype=dtype('float32'), arg_idx=2)],\n",
       "  outs=[BufferSchema(name='C', shape=(1024, 1024), stride=(1024, 1), dtype=dtype('float32'), arg_idx=2)],\n",
       "  pass_configs={},\n",
       "  compile_flags=[],\n",
       "  arg_parser=<function parse_args.<locals>.gemm at 0x7f671c471260>,\n",
       "  const_args=(torch.float16, 1024, 1024, 1024, 1, torch.float16, 1024, 1024, 1024, 1, torch.float32, 128, 128, 128),\n",
       "  prim_func=r'''# from tvm.script import tir as T\n",
       "\n",
       "@T.prim_func\n",
       "def gemm(A_handle: T.handle, B_handle: T.handle, C_handle: T.handle):\n",
       "    A = T.match_buffer(A_handle, (1024, 1024), \"float16\", strides=(1024, 1))\n",
       "    B = T.match_buffer(B_handle, (1024, 1024), \"float16\", strides=(1024, 1))\n",
       "    C = T.match_buffer(C_handle, (1024, 1024), strides=(1024, 1))\n",
       "    # with T.block(\"root\"):\n",
       "    bx = T.launch_thread(\"blockIdx.x\", 8)\n",
       "    by = T.launch_thread(\"blockIdx.y\", 8)\n",
       "    tx = T.launch_thread(\"threadIdx.x\", 128)\n",
       "    ty = T.launch_thread(\"threadIdx.y\", 1)\n",
       "    tz = T.launch_thread(\"threadIdx.z\", 1)\n",
       "    with T.block(\"tilelang_root\"):\n",
       "        T.reads(A[by * 128, 0:897], B[0:897, bx * 128], C[by * 128, bx * 128])\n",
       "        T.writes()\n",
       "        A_shared = T.alloc_buffer((128, 128), \"float16\", scope=\"shared.dyn\")\n",
       "        B_shared = T.alloc_buffer((128, 128), \"float16\", scope=\"shared.dyn\")\n",
       "        C_local = T.alloc_buffer((128, 128), scope=\"local.fragment\")\n",
       "        T.fill(T.tvm_access_ptr(T.type_annotation(\"float32\"), C_local.data, 0, 16384, 2), 0)\n",
       "        for k in T.serial(8, annotations={\"num_stages\": 3}):\n",
       "            T.copy(T.region(A[by * 128, k * 128], 1, 128, 128), T.region(A_shared[0, 0], 2, 128, 128), -1, T.bool(False), 0)\n",
       "            T.copy(T.region(B[k * 128, bx * 128], 1, 128, 128), T.region(B_shared[0, 0], 2, 128, 128), -1, T.bool(False), 0)\n",
       "            T.gemm(T.tvm_access_ptr(T.type_annotation(\"float16\"), A_shared.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation(\"float16\"), B_shared.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation(\"float32\"), C_local.data, 0, 16384, 3), T.bool(False), T.bool(False), 128, 128, 128, 0, T.bool(False), 128, 128, 0, 0, 1, 0, T.uint32(0), 0, 0)\n",
       "        T.copy(T.region(C_local[0, 0], 1, 128, 128), T.region(C[by * 128, bx * 128], 2, 128, 128), -1, T.bool(False), 0)''',\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = gemm.partial(\n",
    "    A=tl.place(1024, 1024, dtype=torch.float16),\n",
    "    B=tl.place(1024, 1024, dtype=torch.float16),\n",
    "    accum_dtype=torch.float32,\n",
    ")\n",
    "func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1e6f5",
   "metadata": {},
   "source": [
    "Please pay attention to the `func.const_args`, it is the cache key used for kernel memory caching\n",
    "\n",
    "It contains\n",
    "* tensor shapes, strides, dtypes\n",
    "* non dynamic kernel parameters\n",
    "* autotune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fba50bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.float16, 1024, 1024, 1024, 1, torch.float16, 1024, 1024, 1024, 1, torch.float32, 128, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(func.const_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066dee5",
   "metadata": {},
   "source": [
    "use `tl.compile` to compile the function, it generate the compiled kernel\n",
    "\n",
    "* kernel.source is the source code of the device code\n",
    "* kernel.wrapped_source contains both device code and host code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01861994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:44:51  [TileLang:tilelang.cache.kernel_cache:WARNING]: Found kernel in memory cache. For better performance, consider using `@tilelang.jit` instead of direct kernel caching.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JITKernel(\n",
       "  lib_path='/home/zhoukexing/.tilelang/cache/28451a0742f2da6d1a7aa58dbb6c9e462bfd75ad6798a7a2b9baedbea38796d5/kernel_lib.so',\n",
       "  lib=<cffi.api._make_ffi_library.<locals>.FFILibrary object at 0x7f6291bffa40>,\n",
       "  lib_call=<function __closure.<locals>.wrapper at 0x7f6292446980>,\n",
       "  source='#include <tl_templates/cuda/gemm.h>\\n#include <tl_templates/cuda/copy.h>\\n#include <tl_templates/cuda/reduce.h>\\n#include <tl_templates/cuda/ldsm.h>\\n#include <tl_templates/cuda/threadblock_swizzle.h>\\n#include <tl_templates/cuda/debug.h>\\n#ifdef ENABLE_BF16\\n#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>\\n#endif\\n\\nextern \"C\" __global__ void gemm_kernel(__grid_constant__ const CUtensorMap A_desc, __grid_constant__ const CUtensorMap B_desc, float* __restrict__ C);\\nextern \"C\" __global__ void __launch_bounds__(256, 1) gemm_kernel(__grid_constant__ const CUtensorMap A_desc, __grid_constant__ const CUtensorMap B_desc, float* __restrict__ C) {\\n  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];\\n  float C_local[128];\\n  __shared__ uint64_t mbarrier_mem[6];\\n  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);\\n  if (tl::tl_shuffle_elect<0>()) {\\n    tl::prefetch_tma_descriptor(A_desc);\\n    tl::prefetch_tma_descriptor(B_desc);\\n    mbarrier[0].init(128);\\n    mbarrier[1].init(128);\\n    mbarrier[2].init(128);\\n    mbarrier[3].init(128);\\n    mbarrier[4].init(128);\\n    mbarrier[5].init(128);\\n  }\\n  __syncthreads();\\n  if (128 <= ((int)threadIdx.x)) {\\n    tl::warpgroup_reg_dealloc<24>();\\n    for (int k = 0; k < 8; ++k) {\\n      mbarrier[((k % 3) + 3)].wait((((k % 6) / 3) ^ 1));\\n      if (tl::tl_shuffle_elect<128>()) {\\n        mbarrier[(k % 3)].expect_transaction(32768);\\n        tl::tma_load(A_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[((k % 3) * 16384)])), (k * 128), (((int)blockIdx.y) * 128));\\n        tl::tma_load(A_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 8192)])), ((k * 128) + 64), (((int)blockIdx.y) * 128));\\n        mbarrier[(k % 3)].expect_transaction(32768);\\n        tl::tma_load(B_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 49152)])), (((int)blockIdx.x) * 128), (k * 128));\\n        tl::tma_load(B_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 57344)])), ((((int)blockIdx.x) * 128) + 64), (k * 128));\\n      }\\n      mbarrier[(k % 3)].arrive();\\n    }\\n  } else {\\n    tl::warpgroup_reg_alloc<240>();\\n    #pragma unroll\\n    for (int i = 0; i < 64; ++i) {\\n      *(float2*)(C_local + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);\\n    }\\n    tl::fence_proxy_async();\\n    for (int k_1 = 0; k_1 < 8; ++k_1) {\\n      mbarrier[(k_1 % 3)].wait(((k_1 % 6) / 3));\\n      tl::gemm_ss<128, 128, 128, 4, 1, 0, 0, 0, 128, 128, 0, 0, true>((&(((half_t*)buf_dyn_shmem)[((k_1 % 3) * 16384)])), (&(((half_t*)buf_dyn_shmem)[(((k_1 % 3) * 16384) + 49152)])), (&(C_local[0])));\\n      mbarrier[((k_1 % 3) + 3)].arrive();\\n    }\\n    #pragma unroll\\n    for (int i_1 = 0; i_1 < 64; ++i_1) {\\n      *(float2*)(C + ((((((((((int)blockIdx.y) * 131072) + ((i_1 >> 5) * 65536)) + ((((int)threadIdx.x) >> 5) * 16384)) + ((i_1 & 1) * 8192)) + (((((int)threadIdx.x) & 31) >> 2) * 1024)) + (((int)blockIdx.x) * 128)) + (((i_1 & 31) >> 1) * 8)) + ((((int)threadIdx.x) & 3) * 2))) = *(float2*)(C_local + (i_1 * 2));\\n    }\\n  }\\n}\\n\\n\\n#define ERROR_BUF_SIZE 1024\\nstatic char error_buf[ERROR_BUF_SIZE];\\n\\nextern \"C\" const char* get_last_error() {\\n    return error_buf;\\n}\\n\\nextern \"C\" int init() {\\n    error_buf[0] = \\'\\\\0\\';\\n    \\n    cudaError_t result_gemm_kernel = cudaFuncSetAttribute(gemm_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 196608);\\n    if (result_gemm_kernel != CUDA_SUCCESS) {\\n        snprintf(error_buf, ERROR_BUF_SIZE, \"Failed to set the allowed dynamic shared memory size to %d with error: %s\", 196608, cudaGetErrorString(result_gemm_kernel));\\n        return -1;\\n    }\\n\\n    return 0;\\n}\\n\\nextern \"C\" int call(half_t* __restrict__ A, half_t* __restrict__ B, float* __restrict__ C, cudaStream_t stream=cudaStreamDefault) {\\n\\n\\tCUtensorMap A_desc;\\n\\tCUtensorMapDataType A_desc_type= (CUtensorMapDataType)6;\\n\\tcuuint32_t A_desc_tensorRank= 2;\\n\\tvoid *A_desc_globalAddress= A;\\n\\tcuuint64_t A_desc_globalDim[2]= {1024,1024};\\n\\tcuuint64_t A_desc_globalStride[2]= {2,2048};\\n\\tcuuint32_t A_desc_boxDim[2]= {64,128};\\n\\tcuuint32_t A_desc_elementStrides[2]= {1,1};\\n\\tCUtensorMapInterleave A_desc_interleave= (CUtensorMapInterleave)0;\\n\\tCUtensorMapSwizzle A_desc_swizzle= (CUtensorMapSwizzle)3;\\n\\tCUtensorMapL2promotion A_desc_l2Promotion= (CUtensorMapL2promotion)2;\\n\\tCUtensorMapFloatOOBfill A_desc_oobFill= (CUtensorMapFloatOOBfill)0;\\n\\n\\tCUresult A_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(\\n    &A_desc, A_desc_type, A_desc_tensorRank, A_desc_globalAddress, A_desc_globalDim, A_desc_globalStride + 1, A_desc_boxDim, A_desc_elementStrides, A_desc_interleave, A_desc_swizzle, A_desc_l2Promotion, A_desc_oobFill);\\n\\n\\tif (A_desc_result != CUDA_SUCCESS) {\\n\\t\\tstd::stringstream ss;\\n\\t\\tss << \"Error: Failed to initialize the TMA descriptor A_desc\";\\n\\t\\tsnprintf(error_buf, ERROR_BUF_SIZE, \"%s\", ss.str().c_str());\\n\\t\\treturn -1;\\n\\t}\\n\\n\\tCUtensorMap B_desc;\\n\\tCUtensorMapDataType B_desc_type= (CUtensorMapDataType)6;\\n\\tcuuint32_t B_desc_tensorRank= 2;\\n\\tvoid *B_desc_globalAddress= B;\\n\\tcuuint64_t B_desc_globalDim[2]= {1024,1024};\\n\\tcuuint64_t B_desc_globalStride[2]= {2,2048};\\n\\tcuuint32_t B_desc_boxDim[2]= {64,128};\\n\\tcuuint32_t B_desc_elementStrides[2]= {1,1};\\n\\tCUtensorMapInterleave B_desc_interleave= (CUtensorMapInterleave)0;\\n\\tCUtensorMapSwizzle B_desc_swizzle= (CUtensorMapSwizzle)3;\\n\\tCUtensorMapL2promotion B_desc_l2Promotion= (CUtensorMapL2promotion)2;\\n\\tCUtensorMapFloatOOBfill B_desc_oobFill= (CUtensorMapFloatOOBfill)0;\\n\\n\\tCUresult B_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(\\n    &B_desc, B_desc_type, B_desc_tensorRank, B_desc_globalAddress, B_desc_globalDim, B_desc_globalStride + 1, B_desc_boxDim, B_desc_elementStrides, B_desc_interleave, B_desc_swizzle, B_desc_l2Promotion, B_desc_oobFill);\\n\\n\\tif (B_desc_result != CUDA_SUCCESS) {\\n\\t\\tstd::stringstream ss;\\n\\t\\tss << \"Error: Failed to initialize the TMA descriptor B_desc\";\\n\\t\\tsnprintf(error_buf, ERROR_BUF_SIZE, \"%s\", ss.str().c_str());\\n\\t\\treturn -1;\\n\\t}\\n\\tgemm_kernel<<<dim3(8, 8, 1), dim3(256, 1, 1), 196608, stream>>>(A_desc, B_desc, C);\\n\\tTILELANG_CHECK_LAST_ERROR(\"gemm_kernel\");\\n\\n\\treturn 0;\\n}\\n',\n",
       "  wrapped_source='#include <tl_templates/cuda/gemm.h>\\n#include <tl_templates/cuda/copy.h>\\n#include <tl_templates/cuda/reduce.h>\\n#include <tl_templates/cuda/ldsm.h>\\n#include <tl_templates/cuda/threadblock_swizzle.h>\\n#include <tl_templates/cuda/debug.h>\\n#ifdef ENABLE_BF16\\n#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>\\n#endif\\n\\nextern \"C\" __global__ void gemm_kernel(__grid_constant__ const CUtensorMap A_desc, __grid_constant__ const CUtensorMap B_desc, float* __restrict__ C);\\nextern \"C\" __global__ void __launch_bounds__(256, 1) gemm_kernel(__grid_constant__ const CUtensorMap A_desc, __grid_constant__ const CUtensorMap B_desc, float* __restrict__ C) {\\n  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];\\n  float C_local[128];\\n  __shared__ uint64_t mbarrier_mem[6];\\n  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);\\n  if (tl::tl_shuffle_elect<0>()) {\\n    tl::prefetch_tma_descriptor(A_desc);\\n    tl::prefetch_tma_descriptor(B_desc);\\n    mbarrier[0].init(128);\\n    mbarrier[1].init(128);\\n    mbarrier[2].init(128);\\n    mbarrier[3].init(128);\\n    mbarrier[4].init(128);\\n    mbarrier[5].init(128);\\n  }\\n  __syncthreads();\\n  if (128 <= ((int)threadIdx.x)) {\\n    tl::warpgroup_reg_dealloc<24>();\\n    for (int k = 0; k < 8; ++k) {\\n      mbarrier[((k % 3) + 3)].wait((((k % 6) / 3) ^ 1));\\n      if (tl::tl_shuffle_elect<128>()) {\\n        mbarrier[(k % 3)].expect_transaction(32768);\\n        tl::tma_load(A_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[((k % 3) * 16384)])), (k * 128), (((int)blockIdx.y) * 128));\\n        tl::tma_load(A_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 8192)])), ((k * 128) + 64), (((int)blockIdx.y) * 128));\\n        mbarrier[(k % 3)].expect_transaction(32768);\\n        tl::tma_load(B_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 49152)])), (((int)blockIdx.x) * 128), (k * 128));\\n        tl::tma_load(B_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 57344)])), ((((int)blockIdx.x) * 128) + 64), (k * 128));\\n      }\\n      mbarrier[(k % 3)].arrive();\\n    }\\n  } else {\\n    tl::warpgroup_reg_alloc<240>();\\n    #pragma unroll\\n    for (int i = 0; i < 64; ++i) {\\n      *(float2*)(C_local + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);\\n    }\\n    tl::fence_proxy_async();\\n    for (int k_1 = 0; k_1 < 8; ++k_1) {\\n      mbarrier[(k_1 % 3)].wait(((k_1 % 6) / 3));\\n      tl::gemm_ss<128, 128, 128, 4, 1, 0, 0, 0, 128, 128, 0, 0, true>((&(((half_t*)buf_dyn_shmem)[((k_1 % 3) * 16384)])), (&(((half_t*)buf_dyn_shmem)[(((k_1 % 3) * 16384) + 49152)])), (&(C_local[0])));\\n      mbarrier[((k_1 % 3) + 3)].arrive();\\n    }\\n    #pragma unroll\\n    for (int i_1 = 0; i_1 < 64; ++i_1) {\\n      *(float2*)(C + ((((((((((int)blockIdx.y) * 131072) + ((i_1 >> 5) * 65536)) + ((((int)threadIdx.x) >> 5) * 16384)) + ((i_1 & 1) * 8192)) + (((((int)threadIdx.x) & 31) >> 2) * 1024)) + (((int)blockIdx.x) * 128)) + (((i_1 & 31) >> 1) * 8)) + ((((int)threadIdx.x) & 3) * 2))) = *(float2*)(C_local + (i_1 * 2));\\n    }\\n  }\\n}\\n\\n\\n#define ERROR_BUF_SIZE 1024\\nstatic char error_buf[ERROR_BUF_SIZE];\\n\\nextern \"C\" const char* get_last_error() {\\n    return error_buf;\\n}\\n\\nextern \"C\" int init() {\\n    error_buf[0] = \\'\\\\0\\';\\n    \\n    cudaError_t result_gemm_kernel = cudaFuncSetAttribute(gemm_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 196608);\\n    if (result_gemm_kernel != CUDA_SUCCESS) {\\n        snprintf(error_buf, ERROR_BUF_SIZE, \"Failed to set the allowed dynamic shared memory size to %d with error: %s\", 196608, cudaGetErrorString(result_gemm_kernel));\\n        return -1;\\n    }\\n\\n    return 0;\\n}\\n\\nextern \"C\" int call(half_t* __restrict__ A, half_t* __restrict__ B, float* __restrict__ C, cudaStream_t stream=cudaStreamDefault) {\\n\\n\\tCUtensorMap A_desc;\\n\\tCUtensorMapDataType A_desc_type= (CUtensorMapDataType)6;\\n\\tcuuint32_t A_desc_tensorRank= 2;\\n\\tvoid *A_desc_globalAddress= A;\\n\\tcuuint64_t A_desc_globalDim[2]= {1024,1024};\\n\\tcuuint64_t A_desc_globalStride[2]= {2,2048};\\n\\tcuuint32_t A_desc_boxDim[2]= {64,128};\\n\\tcuuint32_t A_desc_elementStrides[2]= {1,1};\\n\\tCUtensorMapInterleave A_desc_interleave= (CUtensorMapInterleave)0;\\n\\tCUtensorMapSwizzle A_desc_swizzle= (CUtensorMapSwizzle)3;\\n\\tCUtensorMapL2promotion A_desc_l2Promotion= (CUtensorMapL2promotion)2;\\n\\tCUtensorMapFloatOOBfill A_desc_oobFill= (CUtensorMapFloatOOBfill)0;\\n\\n\\tCUresult A_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(\\n    &A_desc, A_desc_type, A_desc_tensorRank, A_desc_globalAddress, A_desc_globalDim, A_desc_globalStride + 1, A_desc_boxDim, A_desc_elementStrides, A_desc_interleave, A_desc_swizzle, A_desc_l2Promotion, A_desc_oobFill);\\n\\n\\tif (A_desc_result != CUDA_SUCCESS) {\\n\\t\\tstd::stringstream ss;\\n\\t\\tss << \"Error: Failed to initialize the TMA descriptor A_desc\";\\n\\t\\tsnprintf(error_buf, ERROR_BUF_SIZE, \"%s\", ss.str().c_str());\\n\\t\\treturn -1;\\n\\t}\\n\\n\\tCUtensorMap B_desc;\\n\\tCUtensorMapDataType B_desc_type= (CUtensorMapDataType)6;\\n\\tcuuint32_t B_desc_tensorRank= 2;\\n\\tvoid *B_desc_globalAddress= B;\\n\\tcuuint64_t B_desc_globalDim[2]= {1024,1024};\\n\\tcuuint64_t B_desc_globalStride[2]= {2,2048};\\n\\tcuuint32_t B_desc_boxDim[2]= {64,128};\\n\\tcuuint32_t B_desc_elementStrides[2]= {1,1};\\n\\tCUtensorMapInterleave B_desc_interleave= (CUtensorMapInterleave)0;\\n\\tCUtensorMapSwizzle B_desc_swizzle= (CUtensorMapSwizzle)3;\\n\\tCUtensorMapL2promotion B_desc_l2Promotion= (CUtensorMapL2promotion)2;\\n\\tCUtensorMapFloatOOBfill B_desc_oobFill= (CUtensorMapFloatOOBfill)0;\\n\\n\\tCUresult B_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(\\n    &B_desc, B_desc_type, B_desc_tensorRank, B_desc_globalAddress, B_desc_globalDim, B_desc_globalStride + 1, B_desc_boxDim, B_desc_elementStrides, B_desc_interleave, B_desc_swizzle, B_desc_l2Promotion, B_desc_oobFill);\\n\\n\\tif (B_desc_result != CUDA_SUCCESS) {\\n\\t\\tstd::stringstream ss;\\n\\t\\tss << \"Error: Failed to initialize the TMA descriptor B_desc\";\\n\\t\\tsnprintf(error_buf, ERROR_BUF_SIZE, \"%s\", ss.str().c_str());\\n\\t\\treturn -1;\\n\\t}\\n\\tgemm_kernel<<<dim3(8, 8, 1), dim3(256, 1, 1), 196608, stream>>>(A_desc, B_desc, C);\\n\\tTILELANG_CHECK_LAST_ERROR(\"gemm_kernel\");\\n\\n\\treturn 0;\\n}\\n',\n",
       "  jitfunc=JITFunc(\n",
       "    target=cuda -keys=cuda,gpu -arch=sm_90 -max_num_threads=1024 -thread_warp_size=32,\n",
       "    target_host=None,\n",
       "    global_allocs=[BufferSchema(name='C', shape=[1024, 1024], stride=[1024, 1], dtype=dtype('float32'), arg_idx=2)],\n",
       "    outs=[BufferSchema(name='C', shape=(1024, 1024), stride=(1024, 1), dtype=dtype('float32'), arg_idx=2)],\n",
       "    pass_configs={},\n",
       "    compile_flags=[],\n",
       "    arg_parser=<function parse_args.<locals>.gemm at 0x7f671c471260>,\n",
       "    const_args=(torch.float16, 1024, 1024, 1024, 1, torch.float16, 1024, 1024, 1024, 1, torch.float32, 128, 128, 128),\n",
       "    prim_func=r'''# from tvm.script import tir as T\n",
       "\n",
       "@T.prim_func\n",
       "def gemm(A_handle: T.handle, B_handle: T.handle, C_handle: T.handle):\n",
       "    A = T.match_buffer(A_handle, (1024, 1024), \"float16\", strides=(1024, 1))\n",
       "    B = T.match_buffer(B_handle, (1024, 1024), \"float16\", strides=(1024, 1))\n",
       "    C = T.match_buffer(C_handle, (1024, 1024), strides=(1024, 1))\n",
       "    # with T.block(\"root\"):\n",
       "    bx = T.launch_thread(\"blockIdx.x\", 8)\n",
       "    by = T.launch_thread(\"blockIdx.y\", 8)\n",
       "    tx = T.launch_thread(\"threadIdx.x\", 128)\n",
       "    ty = T.launch_thread(\"threadIdx.y\", 1)\n",
       "    tz = T.launch_thread(\"threadIdx.z\", 1)\n",
       "    with T.block(\"tilelang_root\"):\n",
       "        T.reads(A[by * 128, 0:897], B[0:897, bx * 128], C[by * 128, bx * 128])\n",
       "        T.writes()\n",
       "        A_shared = T.alloc_buffer((128, 128), \"float16\", scope=\"shared.dyn\")\n",
       "        B_shared = T.alloc_buffer((128, 128), \"float16\", scope=\"shared.dyn\")\n",
       "        C_local = T.alloc_buffer((128, 128), scope=\"local.fragment\")\n",
       "        T.fill(T.tvm_access_ptr(T.type_annotation(\"float32\"), C_local.data, 0, 16384, 2), 0)\n",
       "        for k in T.serial(8, annotations={\"num_stages\": 3}):\n",
       "            T.copy(T.region(A[by * 128, k * 128], 1, 128, 128), T.region(A_shared[0, 0], 2, 128, 128), -1, T.bool(False), 0)\n",
       "            T.copy(T.region(B[k * 128, bx * 128], 1, 128, 128), T.region(B_shared[0, 0], 2, 128, 128), -1, T.bool(False), 0)\n",
       "            T.gemm(T.tvm_access_ptr(T.type_annotation(\"float16\"), A_shared.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation(\"float16\"), B_shared.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation(\"float32\"), C_local.data, 0, 16384, 3), T.bool(False), T.bool(False), 128, 128, 128, 0, T.bool(False), 128, 128, 0, 0, 1, 0, T.uint32(0), 0, 0)\n",
       "        T.copy(T.region(C_local[0, 0], 1, 128, 128), T.region(C[by * 128, bx * 128], 2, 128, 128), -1, T.bool(False), 0)''',\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = tl.compile(func, verbose=True)\n",
    "kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ebfeb",
   "metadata": {},
   "source": [
    "You can use `kernel.compile` to run both **elaboration** and **compilation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cf7e045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JITKernel(\n",
       "  lib_path='/home/zhoukexing/.tilelang/cache/28451a0742f2da6d1a7aa58dbb6c9e462bfd75ad6798a7a2b9baedbea38796d5/kernel_lib.so',\n",
       "  lib=<cffi.api._make_ffi_library.<locals>.FFILibrary object at 0x7f65e0797aa0>,\n",
       "  lib_call=<function __closure.<locals>.wrapper at 0x7f65e08a5800>,\n",
       "  source='#include <tl_templates/cuda/gemm.h>\\n#include <tl_templates/cuda/copy.h>\\n#include <tl_templates/cuda/reduce.h>\\n#include <tl_templates/cuda/ldsm.h>\\n#include <tl_templates/cuda/threadblock_swizzle.h>\\n#include <tl_templates/cuda/debug.h>\\n#ifdef ENABLE_BF16\\n#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>\\n#endif\\n\\nextern \"C\" __global__ void gemm_kernel(__grid_constant__ const CUtensorMap A_desc, __grid_constant__ const CUtensorMap B_desc, float* __restrict__ C);\\nextern \"C\" __global__ void __launch_bounds__(256, 1) gemm_kernel(__grid_constant__ const CUtensorMap A_desc, __grid_constant__ const CUtensorMap B_desc, float* __restrict__ C) {\\n  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];\\n  float C_local[128];\\n  __shared__ uint64_t mbarrier_mem[6];\\n  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);\\n  if (tl::tl_shuffle_elect<0>()) {\\n    tl::prefetch_tma_descriptor(A_desc);\\n    tl::prefetch_tma_descriptor(B_desc);\\n    mbarrier[0].init(128);\\n    mbarrier[1].init(128);\\n    mbarrier[2].init(128);\\n    mbarrier[3].init(128);\\n    mbarrier[4].init(128);\\n    mbarrier[5].init(128);\\n  }\\n  __syncthreads();\\n  if (128 <= ((int)threadIdx.x)) {\\n    tl::warpgroup_reg_dealloc<24>();\\n    for (int k = 0; k < 8; ++k) {\\n      mbarrier[((k % 3) + 3)].wait((((k % 6) / 3) ^ 1));\\n      if (tl::tl_shuffle_elect<128>()) {\\n        mbarrier[(k % 3)].expect_transaction(32768);\\n        tl::tma_load(A_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[((k % 3) * 16384)])), (k * 128), (((int)blockIdx.y) * 128));\\n        tl::tma_load(A_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 8192)])), ((k * 128) + 64), (((int)blockIdx.y) * 128));\\n        mbarrier[(k % 3)].expect_transaction(32768);\\n        tl::tma_load(B_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 49152)])), (((int)blockIdx.x) * 128), (k * 128));\\n        tl::tma_load(B_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 57344)])), ((((int)blockIdx.x) * 128) + 64), (k * 128));\\n      }\\n      mbarrier[(k % 3)].arrive();\\n    }\\n  } else {\\n    tl::warpgroup_reg_alloc<240>();\\n    #pragma unroll\\n    for (int i = 0; i < 64; ++i) {\\n      *(float2*)(C_local + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);\\n    }\\n    tl::fence_proxy_async();\\n    for (int k_1 = 0; k_1 < 8; ++k_1) {\\n      mbarrier[(k_1 % 3)].wait(((k_1 % 6) / 3));\\n      tl::gemm_ss<128, 128, 128, 4, 1, 0, 0, 0, 128, 128, 0, 0, true>((&(((half_t*)buf_dyn_shmem)[((k_1 % 3) * 16384)])), (&(((half_t*)buf_dyn_shmem)[(((k_1 % 3) * 16384) + 49152)])), (&(C_local[0])));\\n      mbarrier[((k_1 % 3) + 3)].arrive();\\n    }\\n    #pragma unroll\\n    for (int i_1 = 0; i_1 < 64; ++i_1) {\\n      *(float2*)(C + ((((((((((int)blockIdx.y) * 131072) + ((i_1 >> 5) * 65536)) + ((((int)threadIdx.x) >> 5) * 16384)) + ((i_1 & 1) * 8192)) + (((((int)threadIdx.x) & 31) >> 2) * 1024)) + (((int)blockIdx.x) * 128)) + (((i_1 & 31) >> 1) * 8)) + ((((int)threadIdx.x) & 3) * 2))) = *(float2*)(C_local + (i_1 * 2));\\n    }\\n  }\\n}\\n\\n\\n#define ERROR_BUF_SIZE 1024\\nstatic char error_buf[ERROR_BUF_SIZE];\\n\\nextern \"C\" const char* get_last_error() {\\n    return error_buf;\\n}\\n\\nextern \"C\" int init() {\\n    error_buf[0] = \\'\\\\0\\';\\n    \\n    cudaError_t result_gemm_kernel = cudaFuncSetAttribute(gemm_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 196608);\\n    if (result_gemm_kernel != CUDA_SUCCESS) {\\n        snprintf(error_buf, ERROR_BUF_SIZE, \"Failed to set the allowed dynamic shared memory size to %d with error: %s\", 196608, cudaGetErrorString(result_gemm_kernel));\\n        return -1;\\n    }\\n\\n    return 0;\\n}\\n\\nextern \"C\" int call(half_t* __restrict__ A, half_t* __restrict__ B, float* __restrict__ C, cudaStream_t stream=cudaStreamDefault) {\\n\\n\\tCUtensorMap A_desc;\\n\\tCUtensorMapDataType A_desc_type= (CUtensorMapDataType)6;\\n\\tcuuint32_t A_desc_tensorRank= 2;\\n\\tvoid *A_desc_globalAddress= A;\\n\\tcuuint64_t A_desc_globalDim[2]= {1024,1024};\\n\\tcuuint64_t A_desc_globalStride[2]= {2,2048};\\n\\tcuuint32_t A_desc_boxDim[2]= {64,128};\\n\\tcuuint32_t A_desc_elementStrides[2]= {1,1};\\n\\tCUtensorMapInterleave A_desc_interleave= (CUtensorMapInterleave)0;\\n\\tCUtensorMapSwizzle A_desc_swizzle= (CUtensorMapSwizzle)3;\\n\\tCUtensorMapL2promotion A_desc_l2Promotion= (CUtensorMapL2promotion)2;\\n\\tCUtensorMapFloatOOBfill A_desc_oobFill= (CUtensorMapFloatOOBfill)0;\\n\\n\\tCUresult A_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(\\n    &A_desc, A_desc_type, A_desc_tensorRank, A_desc_globalAddress, A_desc_globalDim, A_desc_globalStride + 1, A_desc_boxDim, A_desc_elementStrides, A_desc_interleave, A_desc_swizzle, A_desc_l2Promotion, A_desc_oobFill);\\n\\n\\tif (A_desc_result != CUDA_SUCCESS) {\\n\\t\\tstd::stringstream ss;\\n\\t\\tss << \"Error: Failed to initialize the TMA descriptor A_desc\";\\n\\t\\tsnprintf(error_buf, ERROR_BUF_SIZE, \"%s\", ss.str().c_str());\\n\\t\\treturn -1;\\n\\t}\\n\\n\\tCUtensorMap B_desc;\\n\\tCUtensorMapDataType B_desc_type= (CUtensorMapDataType)6;\\n\\tcuuint32_t B_desc_tensorRank= 2;\\n\\tvoid *B_desc_globalAddress= B;\\n\\tcuuint64_t B_desc_globalDim[2]= {1024,1024};\\n\\tcuuint64_t B_desc_globalStride[2]= {2,2048};\\n\\tcuuint32_t B_desc_boxDim[2]= {64,128};\\n\\tcuuint32_t B_desc_elementStrides[2]= {1,1};\\n\\tCUtensorMapInterleave B_desc_interleave= (CUtensorMapInterleave)0;\\n\\tCUtensorMapSwizzle B_desc_swizzle= (CUtensorMapSwizzle)3;\\n\\tCUtensorMapL2promotion B_desc_l2Promotion= (CUtensorMapL2promotion)2;\\n\\tCUtensorMapFloatOOBfill B_desc_oobFill= (CUtensorMapFloatOOBfill)0;\\n\\n\\tCUresult B_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(\\n    &B_desc, B_desc_type, B_desc_tensorRank, B_desc_globalAddress, B_desc_globalDim, B_desc_globalStride + 1, B_desc_boxDim, B_desc_elementStrides, B_desc_interleave, B_desc_swizzle, B_desc_l2Promotion, B_desc_oobFill);\\n\\n\\tif (B_desc_result != CUDA_SUCCESS) {\\n\\t\\tstd::stringstream ss;\\n\\t\\tss << \"Error: Failed to initialize the TMA descriptor B_desc\";\\n\\t\\tsnprintf(error_buf, ERROR_BUF_SIZE, \"%s\", ss.str().c_str());\\n\\t\\treturn -1;\\n\\t}\\n\\tgemm_kernel<<<dim3(8, 8, 1), dim3(256, 1, 1), 196608, stream>>>(A_desc, B_desc, C);\\n\\tTILELANG_CHECK_LAST_ERROR(\"gemm_kernel\");\\n\\n\\treturn 0;\\n}\\n',\n",
       "  wrapped_source='#include <tl_templates/cuda/gemm.h>\\n#include <tl_templates/cuda/copy.h>\\n#include <tl_templates/cuda/reduce.h>\\n#include <tl_templates/cuda/ldsm.h>\\n#include <tl_templates/cuda/threadblock_swizzle.h>\\n#include <tl_templates/cuda/debug.h>\\n#ifdef ENABLE_BF16\\n#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>\\n#endif\\n\\nextern \"C\" __global__ void gemm_kernel(__grid_constant__ const CUtensorMap A_desc, __grid_constant__ const CUtensorMap B_desc, float* __restrict__ C);\\nextern \"C\" __global__ void __launch_bounds__(256, 1) gemm_kernel(__grid_constant__ const CUtensorMap A_desc, __grid_constant__ const CUtensorMap B_desc, float* __restrict__ C) {\\n  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];\\n  float C_local[128];\\n  __shared__ uint64_t mbarrier_mem[6];\\n  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);\\n  if (tl::tl_shuffle_elect<0>()) {\\n    tl::prefetch_tma_descriptor(A_desc);\\n    tl::prefetch_tma_descriptor(B_desc);\\n    mbarrier[0].init(128);\\n    mbarrier[1].init(128);\\n    mbarrier[2].init(128);\\n    mbarrier[3].init(128);\\n    mbarrier[4].init(128);\\n    mbarrier[5].init(128);\\n  }\\n  __syncthreads();\\n  if (128 <= ((int)threadIdx.x)) {\\n    tl::warpgroup_reg_dealloc<24>();\\n    for (int k = 0; k < 8; ++k) {\\n      mbarrier[((k % 3) + 3)].wait((((k % 6) / 3) ^ 1));\\n      if (tl::tl_shuffle_elect<128>()) {\\n        mbarrier[(k % 3)].expect_transaction(32768);\\n        tl::tma_load(A_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[((k % 3) * 16384)])), (k * 128), (((int)blockIdx.y) * 128));\\n        tl::tma_load(A_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 8192)])), ((k * 128) + 64), (((int)blockIdx.y) * 128));\\n        mbarrier[(k % 3)].expect_transaction(32768);\\n        tl::tma_load(B_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 49152)])), (((int)blockIdx.x) * 128), (k * 128));\\n        tl::tma_load(B_desc, mbarrier[(k % 3)], (&(((half_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 57344)])), ((((int)blockIdx.x) * 128) + 64), (k * 128));\\n      }\\n      mbarrier[(k % 3)].arrive();\\n    }\\n  } else {\\n    tl::warpgroup_reg_alloc<240>();\\n    #pragma unroll\\n    for (int i = 0; i < 64; ++i) {\\n      *(float2*)(C_local + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);\\n    }\\n    tl::fence_proxy_async();\\n    for (int k_1 = 0; k_1 < 8; ++k_1) {\\n      mbarrier[(k_1 % 3)].wait(((k_1 % 6) / 3));\\n      tl::gemm_ss<128, 128, 128, 4, 1, 0, 0, 0, 128, 128, 0, 0, true>((&(((half_t*)buf_dyn_shmem)[((k_1 % 3) * 16384)])), (&(((half_t*)buf_dyn_shmem)[(((k_1 % 3) * 16384) + 49152)])), (&(C_local[0])));\\n      mbarrier[((k_1 % 3) + 3)].arrive();\\n    }\\n    #pragma unroll\\n    for (int i_1 = 0; i_1 < 64; ++i_1) {\\n      *(float2*)(C + ((((((((((int)blockIdx.y) * 131072) + ((i_1 >> 5) * 65536)) + ((((int)threadIdx.x) >> 5) * 16384)) + ((i_1 & 1) * 8192)) + (((((int)threadIdx.x) & 31) >> 2) * 1024)) + (((int)blockIdx.x) * 128)) + (((i_1 & 31) >> 1) * 8)) + ((((int)threadIdx.x) & 3) * 2))) = *(float2*)(C_local + (i_1 * 2));\\n    }\\n  }\\n}\\n\\n\\n#define ERROR_BUF_SIZE 1024\\nstatic char error_buf[ERROR_BUF_SIZE];\\n\\nextern \"C\" const char* get_last_error() {\\n    return error_buf;\\n}\\n\\nextern \"C\" int init() {\\n    error_buf[0] = \\'\\\\0\\';\\n    \\n    cudaError_t result_gemm_kernel = cudaFuncSetAttribute(gemm_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 196608);\\n    if (result_gemm_kernel != CUDA_SUCCESS) {\\n        snprintf(error_buf, ERROR_BUF_SIZE, \"Failed to set the allowed dynamic shared memory size to %d with error: %s\", 196608, cudaGetErrorString(result_gemm_kernel));\\n        return -1;\\n    }\\n\\n    return 0;\\n}\\n\\nextern \"C\" int call(half_t* __restrict__ A, half_t* __restrict__ B, float* __restrict__ C, cudaStream_t stream=cudaStreamDefault) {\\n\\n\\tCUtensorMap A_desc;\\n\\tCUtensorMapDataType A_desc_type= (CUtensorMapDataType)6;\\n\\tcuuint32_t A_desc_tensorRank= 2;\\n\\tvoid *A_desc_globalAddress= A;\\n\\tcuuint64_t A_desc_globalDim[2]= {1024,1024};\\n\\tcuuint64_t A_desc_globalStride[2]= {2,2048};\\n\\tcuuint32_t A_desc_boxDim[2]= {64,128};\\n\\tcuuint32_t A_desc_elementStrides[2]= {1,1};\\n\\tCUtensorMapInterleave A_desc_interleave= (CUtensorMapInterleave)0;\\n\\tCUtensorMapSwizzle A_desc_swizzle= (CUtensorMapSwizzle)3;\\n\\tCUtensorMapL2promotion A_desc_l2Promotion= (CUtensorMapL2promotion)2;\\n\\tCUtensorMapFloatOOBfill A_desc_oobFill= (CUtensorMapFloatOOBfill)0;\\n\\n\\tCUresult A_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(\\n    &A_desc, A_desc_type, A_desc_tensorRank, A_desc_globalAddress, A_desc_globalDim, A_desc_globalStride + 1, A_desc_boxDim, A_desc_elementStrides, A_desc_interleave, A_desc_swizzle, A_desc_l2Promotion, A_desc_oobFill);\\n\\n\\tif (A_desc_result != CUDA_SUCCESS) {\\n\\t\\tstd::stringstream ss;\\n\\t\\tss << \"Error: Failed to initialize the TMA descriptor A_desc\";\\n\\t\\tsnprintf(error_buf, ERROR_BUF_SIZE, \"%s\", ss.str().c_str());\\n\\t\\treturn -1;\\n\\t}\\n\\n\\tCUtensorMap B_desc;\\n\\tCUtensorMapDataType B_desc_type= (CUtensorMapDataType)6;\\n\\tcuuint32_t B_desc_tensorRank= 2;\\n\\tvoid *B_desc_globalAddress= B;\\n\\tcuuint64_t B_desc_globalDim[2]= {1024,1024};\\n\\tcuuint64_t B_desc_globalStride[2]= {2,2048};\\n\\tcuuint32_t B_desc_boxDim[2]= {64,128};\\n\\tcuuint32_t B_desc_elementStrides[2]= {1,1};\\n\\tCUtensorMapInterleave B_desc_interleave= (CUtensorMapInterleave)0;\\n\\tCUtensorMapSwizzle B_desc_swizzle= (CUtensorMapSwizzle)3;\\n\\tCUtensorMapL2promotion B_desc_l2Promotion= (CUtensorMapL2promotion)2;\\n\\tCUtensorMapFloatOOBfill B_desc_oobFill= (CUtensorMapFloatOOBfill)0;\\n\\n\\tCUresult B_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(\\n    &B_desc, B_desc_type, B_desc_tensorRank, B_desc_globalAddress, B_desc_globalDim, B_desc_globalStride + 1, B_desc_boxDim, B_desc_elementStrides, B_desc_interleave, B_desc_swizzle, B_desc_l2Promotion, B_desc_oobFill);\\n\\n\\tif (B_desc_result != CUDA_SUCCESS) {\\n\\t\\tstd::stringstream ss;\\n\\t\\tss << \"Error: Failed to initialize the TMA descriptor B_desc\";\\n\\t\\tsnprintf(error_buf, ERROR_BUF_SIZE, \"%s\", ss.str().c_str());\\n\\t\\treturn -1;\\n\\t}\\n\\tgemm_kernel<<<dim3(8, 8, 1), dim3(256, 1, 1), 196608, stream>>>(A_desc, B_desc, C);\\n\\tTILELANG_CHECK_LAST_ERROR(\"gemm_kernel\");\\n\\n\\treturn 0;\\n}\\n',\n",
       "  jitfunc=JITFunc(\n",
       "    target=cuda -keys=cuda,gpu -arch=sm_90 -max_num_threads=1024 -thread_warp_size=32,\n",
       "    target_host=None,\n",
       "    global_allocs=[BufferSchema(name='C', shape=[1024, 1024], stride=[1024, 1], dtype=dtype('float32'), arg_idx=2)],\n",
       "    outs=[BufferSchema(name='C', shape=(1024, 1024), stride=(1024, 1), dtype=dtype('float32'), arg_idx=2)],\n",
       "    pass_configs={},\n",
       "    compile_flags=[],\n",
       "    arg_parser=<function parse_args.<locals>.gemm at 0x7f671c471260>,\n",
       "    const_args=(torch.float16, 1024, 1024, 1024, 1, torch.float16, 1024, 1024, 1024, 1, torch.float32, 128, 128, 128),\n",
       "    prim_func=r'''# from tvm.script import tir as T\n",
       "\n",
       "@T.prim_func\n",
       "def gemm(A_handle: T.handle, B_handle: T.handle, C_handle: T.handle):\n",
       "    A = T.match_buffer(A_handle, (1024, 1024), \"float16\", strides=(1024, 1))\n",
       "    B = T.match_buffer(B_handle, (1024, 1024), \"float16\", strides=(1024, 1))\n",
       "    C = T.match_buffer(C_handle, (1024, 1024), strides=(1024, 1))\n",
       "    # with T.block(\"root\"):\n",
       "    bx = T.launch_thread(\"blockIdx.x\", 8)\n",
       "    by = T.launch_thread(\"blockIdx.y\", 8)\n",
       "    tx = T.launch_thread(\"threadIdx.x\", 128)\n",
       "    ty = T.launch_thread(\"threadIdx.y\", 1)\n",
       "    tz = T.launch_thread(\"threadIdx.z\", 1)\n",
       "    with T.block(\"tilelang_root\"):\n",
       "        T.reads(A[by * 128, 0:897], B[0:897, bx * 128], C[by * 128, bx * 128])\n",
       "        T.writes()\n",
       "        A_shared = T.alloc_buffer((128, 128), \"float16\", scope=\"shared.dyn\")\n",
       "        B_shared = T.alloc_buffer((128, 128), \"float16\", scope=\"shared.dyn\")\n",
       "        C_local = T.alloc_buffer((128, 128), scope=\"local.fragment\")\n",
       "        T.fill(T.tvm_access_ptr(T.type_annotation(\"float32\"), C_local.data, 0, 16384, 2), 0)\n",
       "        for k in T.serial(8, annotations={\"num_stages\": 3}):\n",
       "            T.copy(T.region(A[by * 128, k * 128], 1, 128, 128), T.region(A_shared[0, 0], 2, 128, 128), -1, T.bool(False), 0)\n",
       "            T.copy(T.region(B[k * 128, bx * 128], 1, 128, 128), T.region(B_shared[0, 0], 2, 128, 128), -1, T.bool(False), 0)\n",
       "            T.gemm(T.tvm_access_ptr(T.type_annotation(\"float16\"), A_shared.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation(\"float16\"), B_shared.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation(\"float32\"), C_local.data, 0, 16384, 3), T.bool(False), T.bool(False), 128, 128, 128, 0, T.bool(False), 128, 128, 0, 0, 1, 0, T.uint32(0), 0, 0)\n",
       "        T.copy(T.region(C_local[0, 0], 1, 128, 128), T.region(C[by * 128, bx * 128], 2, 128, 128), -1, T.bool(False), 0)''',\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = gemm.compile(\n",
    "    A=tl.place(1024, 1024, dtype=torch.float16),\n",
    "    B=tl.place(1024, 1024, dtype=torch.float16),\n",
    "    accum_dtype=torch.float32,\n",
    ")\n",
    "kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e4ed8",
   "metadata": {},
   "source": [
    "Placeholder is not a valid tensor, if you call the kernel with placeholder, it raises error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01f15797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError('an integer is required')\n"
     ]
    }
   ],
   "source": [
    "A = tl.place(1024, 1024, dtype=torch.float16)\n",
    "B = tl.place(1024, 1024, dtype=torch.float16)\n",
    "try:\n",
    "    C = gemm(A, B)\n",
    "except TypeError as e:\n",
    "    print(repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be73d58",
   "metadata": {},
   "source": [
    "You can manually parse the arguments using kernel.parse_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abaa8eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const_args:  (torch.float32, 1024, 1024, 1024, 1, torch.float32, 1024, 1024, 1024, 1, torch.float32, 128, 128, 128)\n",
      "dyn_args:    (empty_data_ptr(), empty_data_ptr(), None, 0)\n"
     ]
    }
   ],
   "source": [
    "const_args, dyn_args = gemm.arg_parser(\n",
    "    tl.place(1024, 1024, dtype=torch.float32),\n",
    "    tl.place(1024, 1024, dtype=torch.float32)\n",
    ")\n",
    "print('const_args: ', const_args)\n",
    "print('dyn_args:   ', dyn_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29cf6b2",
   "metadata": {},
   "source": [
    "## Migration from JITv1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20ed3c",
   "metadata": {},
   "source": [
    "JITv2 support almost all grammar in JITv1, the only difference is the signature of function\n",
    "\n",
    "When migrating from JITv1 to JITv2\n",
    "1. Rewrite the function signature\n",
    "2. Add shape checking\n",
    "3. Allocate global buffer\n",
    "4. Copy all `tl.Kernel` inside\n",
    "\n",
    "Here is a example to migrate `flashattn_fwd` to JITv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8269848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tilelang.language as T\n",
    "import tilelang.language.v2 as tl\n",
    "import tilelang\n",
    "from tilelang import PassConfigKey\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "816148ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tilelang.jit(\n",
    "    out_idx=[3, 4], pass_configs={\n",
    "        tilelang.PassConfigKey.TL_ENABLE_FAST_MATH: True,\n",
    "    })\n",
    "def flashattn_fwd_jitv1(batch, heads, seq_len, dim_qk, dim_v, is_causal, block_M, block_N, groups=1):\n",
    "    scale = (1.0 / dim_qk)**0.5 * 1.44269504  # log2(e)\n",
    "    head_kv = heads // groups\n",
    "    q_shape = [batch, seq_len, heads, dim_qk]\n",
    "    k_shape = [batch, seq_len, head_kv, dim_qk]\n",
    "    v_shape = [batch, seq_len, head_kv, dim_v]\n",
    "    dtype = \"float16\"\n",
    "    accum_dtype = \"float\"\n",
    "\n",
    "    @T.prim_func\n",
    "    def flash_fwd(\n",
    "            Q: T.Tensor(q_shape, dtype),  # type: ignore\n",
    "            K: T.Tensor(k_shape, dtype),  # type: ignore\n",
    "            V: T.Tensor(v_shape, dtype),  # type: ignore\n",
    "            Output: T.Tensor([batch, seq_len, heads, dim_v], dtype),  # type: ignore\n",
    "            lse: T.Tensor([batch, heads, seq_len], accum_dtype),  # type: ignore\n",
    "    ):\n",
    "        with T.Kernel(T.ceildiv(seq_len, block_M), heads, batch, threads=256) as (bx, by, bz):\n",
    "            Q_shared = T.alloc_shared([block_M, dim_qk], dtype)\n",
    "            K_shared = T.alloc_shared([block_N, dim_qk], dtype)\n",
    "            V_shared = T.alloc_shared([block_N, dim_v], dtype)\n",
    "            acc_s = T.alloc_fragment([block_M, block_N], accum_dtype)\n",
    "            acc_s_cast = T.alloc_fragment([block_M, block_N], dtype)\n",
    "            acc_o = T.alloc_fragment([block_M, dim_v], accum_dtype)\n",
    "            scores_max = T.alloc_fragment([block_M], accum_dtype)\n",
    "            scores_max_prev = T.alloc_fragment([block_M], accum_dtype)\n",
    "            scores_scale = T.alloc_fragment([block_M], accum_dtype)\n",
    "            scores_sum = T.alloc_fragment([block_M], accum_dtype)\n",
    "            logsum = T.alloc_fragment([block_M], accum_dtype)\n",
    "\n",
    "            T.annotate_layout({Q_shared: tilelang.layout.make_swizzled_layout(Q_shared)})\n",
    "            T.copy(Q[bz, bx * block_M:(bx + 1) * block_M, by, :], Q_shared)\n",
    "            T.fill(acc_o, 0)\n",
    "            T.fill(logsum, 0)\n",
    "            T.fill(scores_max, -T.infinity(accum_dtype))\n",
    "            loop_range = (\n",
    "                T.ceildiv(\n",
    "                    (bx + 1) * block_M, block_N) if is_causal else T.ceildiv(seq_len, block_N))\n",
    "            for k in T.Pipelined(loop_range, num_stages=1):\n",
    "                T.copy(K[bz, k * block_N:(k + 1) * block_N, by // groups, :], K_shared)\n",
    "                if is_causal:\n",
    "                    for i, j in T.Parallel(block_M, block_N):\n",
    "                        acc_s[i, j] = T.if_then_else(bx * block_M + i >= k * block_N + j, 0,\n",
    "                                                     -T.infinity(acc_s.dtype))\n",
    "                else:\n",
    "                    T.clear(acc_s)\n",
    "                T.gemm(Q_shared, K_shared, acc_s, transpose_B=True, policy=T.GemmWarpPolicy.FullRow)\n",
    "                T.copy(V[bz, k * block_N:(k + 1) * block_N, by // groups, :], V_shared)\n",
    "                T.copy(scores_max, scores_max_prev)\n",
    "                T.reduce_max(acc_s, scores_max, dim=1, clear=False)\n",
    "                for i in T.Parallel(block_M):\n",
    "                    scores_scale[i] = T.exp2(scores_max_prev[i] * scale - scores_max[i] * scale)\n",
    "                for i, j in T.Parallel(block_M, dim_v):\n",
    "                    acc_o[i, j] *= scores_scale[i]\n",
    "                for i, j in T.Parallel(block_M, block_N):\n",
    "                    acc_s[i, j] = T.exp2(acc_s[i, j] * scale - scores_max[i] * scale)\n",
    "                T.copy(acc_s, acc_s_cast)\n",
    "                T.gemm(acc_s_cast, V_shared, acc_o, policy=T.GemmWarpPolicy.FullRow)\n",
    "                T.reduce_sum(acc_s, scores_sum, dim=1)\n",
    "                for i in T.Parallel(block_M):\n",
    "                    logsum[i] = logsum[i] * scores_scale[i] + scores_sum[i]\n",
    "            for i, j in T.Parallel(block_M, dim_v):\n",
    "                acc_o[i, j] /= logsum[i]\n",
    "            T.copy(acc_o, Output[bz, bx * block_M:(bx + 1) * block_M, by, :])\n",
    "            for i in T.Parallel(block_M):\n",
    "                logsum[i] = T.log2(logsum[i]) + scores_max[i] * scale\n",
    "            T.copy(logsum, lse[bz, by, bx * block_M:(bx + 1) * block_M])\n",
    "\n",
    "    return flash_fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09d05e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_configs = {\n",
    "    PassConfigKey.TL_ENABLE_FAST_MATH: True\n",
    "}\n",
    "@tl.jit(pass_configs=pass_configs)\n",
    "def flashattn_fwd_jitv2(\n",
    "    Q: tl.Tensor[int, int, int, int],\n",
    "    K: tl.Tensor[int, int, int, int],\n",
    "    V: tl.Tensor[int, int, int, int],\n",
    "    is_causal: bool,\n",
    "    block_M: int=128,\n",
    "    block_N: int=64,\n",
    "    accum_dtype: torch.dtype = torch.float32,\n",
    "    threads: int = 256,\n",
    "    groups: int = 1\n",
    "):\n",
    "\n",
    "    # 1. extract shape and dtype\n",
    "    batch, seq_len, heads, dim_qk, dtype = Q.params()\n",
    "    batch, seq_len, heads_kv, dim_qk, dtype = K.params()\n",
    "    batch, seq_len, heads_kv, dim_v, dtype = V.params()\n",
    "\n",
    "    # 3. allocate output\n",
    "    Output = tl.empty((batch, seq_len, heads, dim_v), dtype)\n",
    "    lse = tl.empty((batch, heads, seq_len), accum_dtype)\n",
    "\n",
    "    scale = (1.0 / dim_qk)**0.5 * 1.44269504  # log2(e)\n",
    "\n",
    "    # 4. paste all tilelang v1 code here\n",
    "    with tl.Kernel(tl.ceildiv(seq_len, block_M), heads, batch, threads=threads) as (bx, by, bz):\n",
    "        Q_shared = T.alloc_shared([block_M, dim_qk], dtype)\n",
    "        K_shared = T.alloc_shared([block_N, dim_qk], dtype)\n",
    "        V_shared = T.alloc_shared([block_N, dim_v], dtype)\n",
    "        acc_s = T.alloc_fragment([block_M, block_N], accum_dtype)\n",
    "        acc_s_cast = T.alloc_fragment([block_M, block_N], dtype)\n",
    "        acc_o = T.alloc_fragment([block_M, dim_v], accum_dtype)\n",
    "        scores_max = T.alloc_fragment([block_M], accum_dtype)\n",
    "        scores_max_prev = T.alloc_fragment([block_M], accum_dtype)\n",
    "        scores_scale = T.alloc_fragment([block_M], accum_dtype)\n",
    "        scores_sum = T.alloc_fragment([block_M], accum_dtype)\n",
    "        logsum = T.alloc_fragment([block_M], accum_dtype)\n",
    "\n",
    "        T.annotate_layout({Q_shared: tilelang.layout.make_swizzled_layout(Q_shared)})\n",
    "        T.copy(Q[bz, bx * block_M:(bx + 1) * block_M, by, :], Q_shared)\n",
    "        T.fill(acc_o, 0)\n",
    "        T.fill(logsum, 0)\n",
    "        T.fill(scores_max, -T.infinity(accum_dtype))\n",
    "        loop_range = (\n",
    "            T.ceildiv(\n",
    "                (bx + 1) * block_M, block_N) if is_causal else T.ceildiv(seq_len, block_N))\n",
    "        for k in T.Pipelined(loop_range, num_stages=1):\n",
    "            T.copy(K[bz, k * block_N:(k + 1) * block_N, by // groups, :], K_shared)\n",
    "            if is_causal:\n",
    "                for i, j in T.Parallel(block_M, block_N):\n",
    "                    acc_s[i, j] = T.if_then_else(bx * block_M + i >= k * block_N + j, 0,\n",
    "                                                    -T.infinity(acc_s.dtype))\n",
    "            else:\n",
    "                T.clear(acc_s)\n",
    "            T.gemm(Q_shared, K_shared, acc_s, transpose_B=True, policy=T.GemmWarpPolicy.FullRow)\n",
    "            T.copy(V[bz, k * block_N:(k + 1) * block_N, by // groups, :], V_shared)\n",
    "            T.copy(scores_max, scores_max_prev)\n",
    "            T.reduce_max(acc_s, scores_max, dim=1, clear=False)\n",
    "            for i in T.Parallel(block_M):\n",
    "                scores_scale[i] = T.exp2(scores_max_prev[i] * scale - scores_max[i] * scale)\n",
    "            for i, j in T.Parallel(block_M, dim_v):\n",
    "                acc_o[i, j] *= scores_scale[i]\n",
    "            for i, j in T.Parallel(block_M, block_N):\n",
    "                acc_s[i, j] = T.exp2(acc_s[i, j] * scale - scores_max[i] * scale)\n",
    "            T.copy(acc_s, acc_s_cast)\n",
    "            T.gemm(acc_s_cast, V_shared, acc_o, policy=T.GemmWarpPolicy.FullRow)\n",
    "            T.reduce_sum(acc_s, scores_sum, dim=1)\n",
    "            for i in T.Parallel(block_M):\n",
    "                logsum[i] = logsum[i] * scores_scale[i] + scores_sum[i]\n",
    "        for i, j in T.Parallel(block_M, dim_v):\n",
    "            acc_o[i, j] /= logsum[i]\n",
    "        T.copy(acc_o, Output[bz, bx * block_M:(bx + 1) * block_M, by, :])\n",
    "        for i in T.Parallel(block_M):\n",
    "            logsum[i] = T.log2(logsum[i]) + scores_max[i] * scale\n",
    "        T.copy(logsum, lse[bz, by, bx * block_M:(bx + 1) * block_M])\n",
    "\n",
    "    return Output, lse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d720ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH: int = 1\n",
    "H: int = 32\n",
    "N_CTX: int = 256\n",
    "D_HEAD_QK: int = 192\n",
    "D_HEAD_V: int = 128\n",
    "groups: int = 16\n",
    "causal: bool = False\n",
    "use_atomic: bool = True\n",
    "head_kv = H // groups\n",
    "Q = torch.randn(BATCH, N_CTX, H, D_HEAD_QK, dtype=torch.half, device=\"cuda\")\n",
    "K = torch.randn(BATCH, N_CTX, head_kv, D_HEAD_QK, dtype=torch.half, device=\"cuda\")\n",
    "V = torch.randn(BATCH, N_CTX, head_kv, D_HEAD_V, dtype=torch.half, device=\"cuda\")\n",
    "batch, seq_len, heads, dim_qk = Q.shape\n",
    "batch, seq_len, heads_kv, dim_qk = K.shape\n",
    "batch, seq_len, heads_kv, dim_v = V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3780b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:44:58  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `flash_fwd` with `out_idx=[3, 4]`\n",
      "2025-10-13 15:45:09  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `flash_fwd`\n",
      "2025-10-13 15:45:10  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `flashattn_fwd_jitv2` with `out_idx=[3, 4]`\n",
      "2025-10-13 15:45:21  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `flashattn_fwd_jitv2`\n",
      "JITv1:  33.56990269385278 us\n",
      "JITv2:  16.764699900522828 us\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "jit_v1_kernel = flashattn_fwd_jitv1(batch, heads, seq_len, dim_qk, dim_v, causal, block_M=128, block_N=128, groups=groups)\n",
    "flashattn_fwd_jitv2(Q, K, V, causal, block_M=128, block_N=128, groups=groups)\n",
    "\n",
    "time_beg = time.perf_counter()\n",
    "for _ in range(10000):\n",
    "    res_v1 = jit_v1_kernel(Q, K, V)\n",
    "time_end = time.perf_counter()\n",
    "print('JITv1: ', (time_end - time_beg) / 10000 * 1e6, 'us')\n",
    "\n",
    "time_beg = time.perf_counter()\n",
    "for _ in range(10000):\n",
    "    res_v2 = flashattn_fwd_jitv2(Q, K, V, causal, block_M=128, block_N=128, groups=groups)\n",
    "time_end = time.perf_counter()\n",
    "print('JITv2: ', (time_end - time_beg) / 10000 * 1e6, 'us')\n",
    "torch.testing.assert_close(res_v1, res_v2, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65c4b988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013378110394842798"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flashattn_fwd_jitv2.bench(Q, K, V, causal, block_M=128, block_N=128, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d82736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kexing-test_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
