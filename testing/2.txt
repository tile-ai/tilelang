============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0
rootdir: /workspace/tilelang
configfile: pyproject.toml
plugins: anyio-4.6.2.post1, xdist-3.6.1, asyncio-0.24.0, forked-1.6.0, rerunfailures-14.0, buildkite-test-collector-0.1.9, shard-0.1.2, mock-3.14.0
asyncio: mode=strict, default_loop_scope=None
collected 1 item
Running 1 items in this shard

python/kernel/test_tilelang_kernel_mha_bwd.py ------------
 # from tvm.script import tir as T

@T.prim_func
def flash_fwd(Q: T.Buffer((8, 1024, 32, 64), "float16"), K: T.Buffer((8, 1024, 32, 64), "float16"), V: T.Buffer((8, 1024, 32, 64), "float16"), Output: T.Buffer((8, 1024, 32, 64), "float16"), lse: T.Buffer((8, 32, 1024), "float32")):
    # with T.block("root"):
    bx = T.launch_thread("blockIdx.x", 16)
    by = T.launch_thread("blockIdx.y", 32)
    bz = T.launch_thread("blockIdx.z", 8)
    tx = T.launch_thread("threadIdx.x", 32)
    ty = T.launch_thread("threadIdx.y", 1)
    tz = T.launch_thread("threadIdx.z", 1)
    with T.block(""):
        loop_range = T.int32()
        T.reads(Q[bz, bx * 64, by, 0], K[bz, 0:loop_range * 64 - 63, by, 0], V[bz, 0:loop_range * 64 - 63, by, 0], Output[bz, bx * 64, by, 0], lse[bz, by, bx * 64])
        T.writes()
        Q_shared = T.handle("float16", "shared.dyn")
        T.block_attr({"layout_map": {{Q_shared: metadata["tl.Layout"][0]}}})
        Q_shared_1 = T.alloc_buffer((64, 64), "float16", data=Q_shared, scope="shared.dyn")
        K_shared = T.alloc_buffer((64, 64), "float16", scope="shared.dyn")
        V_shared = T.alloc_buffer((64, 64), "float16", scope="shared.dyn")
        acc_s = T.alloc_buffer((64, 64), scope="local.fragment")
        acc_s_cast = T.alloc_buffer((64, 64), "float16", scope="local.fragment")
        acc_o = T.alloc_buffer((64, 64), scope="local.fragment")
        scores_max = T.alloc_buffer((64,), scope="local.fragment")
        scores_max_prev = T.alloc_buffer((64,), scope="local.fragment")
        scores_scale = T.alloc_buffer((64,), scope="local.fragment")
        scores_sum = T.alloc_buffer((64,), scope="local.fragment")
        logsum = T.alloc_buffer((64,), scope="local.fragment")
        T.copy(T.region(Q[bz, bx * 64, by, 0], 1, 1, 64, 1, 64), T.region(Q_shared_1[0, 0], 2, 64, 64))
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), acc_o.data, 0, 4096, 2), 0)
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), logsum.data, 0, 64, 2), 0)
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), scores_max.data, 0, 64, 2), T.float32("-inf"))
        with T.LetStmt(16, var=loop_range):
            for k in range(loop_range):
                T.copy(T.region(K[bz, k * 64, by, 0], 1, 1, 64, 1, 64), T.region(K_shared[0, 0], 2, 64, 64))
                T.fill(T.tvm_access_ptr(T.type_annotation("float32"), acc_s.data, 0, 4096, 2), 0)
                T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), Q_shared, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float16"), K_shared.data, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float32"), acc_s.data, 0, 4096, 3), T.bool(False), T.bool(True), 64, 64, 64, 1, T.bool(False), 1, 0)
                T.copy(T.region(V[bz, k * 64, by, 0], 1, 1, 64, 1, 64), T.region(V_shared[0, 0], 2, 64, 64))
                T.copy(T.region(scores_max[0], 1, 64), T.region(scores_max_prev[0], 2, 64))
                T.reduce(T.tvm_access_ptr(T.type_annotation("float32"), acc_s.data, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float32"), scores_max.data, 0, 64, 2), "max", 1, T.bool(False))
                for i in T.parallel(64):
                    scores_scale[i] = T.exp2(scores_max_prev[i] * T.float32(0.18033688) - scores_max[i] * T.float32(0.18033688))
                for i in T.parallel(64):
                    for j in T.parallel(64):
                        acc_o[i, j] = acc_o[i, j] * scores_scale[i]
                for i in T.parallel(64):
                    for j in T.parallel(64):
                        acc_s[i, j] = T.exp2(acc_s[i, j] * T.float32(0.18033688) - scores_max[i] * T.float32(0.18033688))
                T.copy(T.region(acc_s[0, 0], 1, 64, 64), T.region(acc_s_cast[0, 0], 2, 64, 64))
                T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), acc_s_cast.data, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float16"), V_shared.data, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float32"), acc_o.data, 0, 4096, 3), T.bool(False), T.bool(False), 64, 64, 64, 1, T.bool(False), 1, 0)
                T.reduce(T.tvm_access_ptr(T.type_annotation("float32"), acc_s.data, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float32"), scores_sum.data, 0, 64, 2), "sum", 1, T.bool(True))
                for i in T.parallel(64):
                    logsum[i] = logsum[i] * scores_scale[i] + scores_sum[i]
            for i in T.parallel(64):
                for j in T.parallel(64):
                    acc_o[i, j] = acc_o[i, j] / logsum[i]
            T.copy(T.region(acc_o[0, 0], 1, 64, 64), T.region(Output[bz, bx * 64, by, 0], 2, 1, 64, 1, 64))
            for i in T.parallel(64):
                logsum[i] = T.log2(logsum[i]) + scores_max[i] * T.float32(0.18033688)
            T.copy(T.region(logsum[0], 1, 64), T.region(lse[bz, by, bx * 64], 2, 1, 1, 64))

# Metadata omitted. Use show_meta=True in script() method to show it. ------------

49761ebb8fe0a01df602f63c97022e8e2f7e0d468b846e8ee50294f8d7da504d
------------
 # from tvm.script import tir as T

@T.prim_func
def flash_bwd_prep(O: T.Buffer((8, 1024, 32, 64), "float16"), dO: T.Buffer((8, 1024, 32, 64), "float16"), Delta: T.Buffer((8, 32, 1024), "float32")):
    # with T.block("root"):
    bx = T.launch_thread("blockIdx.x", 32)
    by = T.launch_thread("blockIdx.y", 32)
    bz = T.launch_thread("blockIdx.z", 8)
    tx = T.launch_thread("threadIdx.x", 128)
    ty = T.launch_thread("threadIdx.y", 1)
    tz = T.launch_thread("threadIdx.z", 1)
    with T.block(""):
        T.reads(O[bz, by * 32, bx, 0:33], dO[bz, by * 32, bx, 0:33], Delta[bz, bx, by * 32])
        T.writes()
        o = T.alloc_buffer((32, 32), "float16", scope="local.fragment")
        do = T.alloc_buffer((32, 32), "float16", scope="local.fragment")
        acc = T.alloc_buffer((32, 32), scope="local.fragment")
        delta = T.alloc_buffer((32,), scope="local.fragment")
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), acc.data, 0, 1024, 2), 0)
        for k in range(2):
            T.copy(T.region(O[bz, by * 32, bx, k * 32], 1, 1, 32, 1, 32), T.region(o[0, 0], 2, 32, 32))
            T.copy(T.region(dO[bz, by * 32, bx, k * 32], 1, 1, 32, 1, 32), T.region(do[0, 0], 2, 32, 32))
            for i in T.parallel(32):
                for j in T.parallel(32):
                    acc[i, j] = acc[i, j] + T.Cast("float32", o[i, j] * do[i, j])
        T.reduce(T.tvm_access_ptr(T.type_annotation("float32"), acc.data, 0, 1024, 1), T.tvm_access_ptr(T.type_annotation("float32"), delta.data, 0, 32, 2), "sum", 1, T.bool(True))
        T.copy(T.region(delta[0], 1, 32), T.region(Delta[bz, bx, by * 32], 2, 1, 1, 32)) ------------

a403fb01108aaa52a46b8473024e9d69682958197e37d8facf5cfc3fb004bdd1
------------
 # from tvm.script import tir as T

@T.prim_func
def flash_bwd_post(dQ: T.Buffer((8, 1024, 32, 64), "float32"), dQ_out: T.Buffer((8, 1024, 32, 64), "float16")):
    # with T.block("root"):
    bx = T.launch_thread("blockIdx.x", 16)
    by = T.launch_thread("blockIdx.y", 32)
    bz = T.launch_thread("blockIdx.z", 8)
    tx = T.launch_thread("threadIdx.x", 128)
    ty = T.launch_thread("threadIdx.y", 1)
    tz = T.launch_thread("threadIdx.z", 1)
    with T.block(""):
        T.reads(dQ[bz, bx * 64, by, 0], dQ_out[bz, bx * 64, by, 0])
        T.writes()
        T.block_attr({"layout_map": {{dQ.data: metadata["tl.Layout"][0]}}})
        T.copy(T.region(dQ[bz, bx * 64, by, 0], 1, 1, 64, 1, 64), T.region(dQ_out[bz, bx * 64, by, 0], 2, 1, 64, 1, 64))

# Metadata omitted. Use show_meta=True in script() method to show it. ------------

c5d1256213ca0e50867cb789d1bb40ca307e29cff04358959f772528ca7ac61b
------------
 # from tvm.script import tir as T

@T.prim_func
def flash_bwd(Q: T.Buffer((8, 1024, 32, 64), "float16"), K: T.Buffer((8, 1024, 32, 64), "float16"), V: T.Buffer((8, 1024, 32, 64), "float16"), dO: T.Buffer((8, 1024, 32, 64), "float16"), lse: T.Buffer((8, 32, 1024), "float32"), Delta: T.Buffer((8, 32, 1024), "float32"), dQ: T.Buffer((8, 1024, 32, 64), "float32"), dK: T.Buffer((8, 1024, 32, 64), "float16"), dV: T.Buffer((8, 1024, 32, 64), "float16")):
    # with T.block("root"):
    bx = T.launch_thread("blockIdx.x", 32)
    by = T.launch_thread("blockIdx.y", 8)
    bz = T.launch_thread("blockIdx.z", 8)
    tx = T.launch_thread("threadIdx.x", 32)
    ty = T.launch_thread("threadIdx.y", 1)
    tz = T.launch_thread("threadIdx.z", 1)
    with T.block(""):
        loop_st = T.int32()
        loop_ed = T.int32()
        T.reads(K[bz, by * 128, bx, 0], V[bz, by * 128, bx, 0], Q[bz, loop_st * 128:loop_st * 128 + (loop_ed * 128 - loop_st * 128 - 127), bx, 0], lse[bz, bx, loop_st * 128:loop_st * 128 + (loop_ed * 128 - loop_st * 128 - 127)], dO[bz, loop_st * 128:loop_st * 128 + (loop_ed * 128 - loop_st * 128 - 127), bx, 0], Delta[bz, bx, loop_st * 128:loop_st * 128 + (loop_ed * 128 - loop_st * 128 - 127)], dQ[bz, loop_st * 128:loop_st * 128 + (loop_ed * 128 - loop_st * 128), bx, 0:64], dV[bz, by * 128, bx, 0], dK[bz, by * 128, bx, 0])
        T.writes()
        K_shared = T.handle("float16", "shared.dyn")
        dk_shared = T.handle("float16", "shared.dyn")
        dv_shared = T.handle("float16", "shared.dyn")
        T.block_attr({"layout_map": {{K_shared: metadata["tl.Layout"][0], dQ.data: metadata["tl.Layout"][1], dk_shared: metadata["tl.Layout"][2], dv_shared: metadata["tl.Layout"][3]}}})
        K_shared_1 = T.alloc_buffer((128, 64), "float16", data=K_shared, scope="shared.dyn")
        dsT_shared = T.alloc_buffer((128, 128), "float16", scope="shared.dyn")
        q = T.alloc_buffer((128, 64), "float16", scope="shared.dyn")
        V_shared = T.alloc_buffer((128, 64), "float16", scope="shared.dyn")
        qkT = T.alloc_buffer((128, 128), scope="local.fragment")
        dsT = T.alloc_buffer((128, 128), scope="local.fragment")
        qkT_cast = T.alloc_buffer((128, 128), "float16", scope="local.fragment")
        dsT_cast = T.alloc_buffer((128, 128), "float16", scope="local.fragment")
        lse_shared = T.alloc_buffer((128,), scope="shared.dyn")
        delta = T.alloc_buffer((128,), scope="shared.dyn")
        do = T.alloc_buffer((128, 64), "float16", scope="shared.dyn")
        dv = T.alloc_buffer((128, 64), scope="local.fragment")
        dk = T.alloc_buffer((128, 64), scope="local.fragment")
        dq = T.alloc_buffer((128, 64), scope="local.fragment")
        dv_shared_1 = T.alloc_buffer((128, 64), "float16", data=dv_shared, scope="shared.dyn")
        dk_shared_1 = T.alloc_buffer((128, 64), "float16", data=dk_shared, scope="shared.dyn")
        T.copy(T.region(K[bz, by * 128, bx, 0], 1, 1, 128, 1, 64), T.region(K_shared_1[0, 0], 2, 128, 64))
        T.copy(T.region(V[bz, by * 128, bx, 0], 1, 1, 128, 1, 64), T.region(V_shared[0, 0], 2, 128, 64))
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), dv.data, 0, 8192, 2), 0)
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), dk.data, 0, 8192, 2), 0)
        with T.LetStmt(0, var=loop_st):
            with T.LetStmt(8, var=loop_ed):
                for k in range(loop_st, loop_st + (loop_ed - loop_st)):
                    T.copy(T.region(Q[bz, k * 128, bx, 0], 1, 1, 128, 1, 64), T.region(q[0, 0], 2, 128, 64))
                    T.fill(T.tvm_access_ptr(T.type_annotation("float32"), qkT.data, 0, 16384, 2), 0)
                    T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), K_shared, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float16"), q.data, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float32"), qkT.data, 0, 16384, 3), T.bool(False), T.bool(True), 128, 128, 64, 1, T.bool(False), 1, 0)
                    T.copy(T.region(lse[bz, bx, k * 128], 1, 1, 1, 128), T.region(lse_shared[0], 2, 128))
                    for i in T.parallel(128):
                        for j in T.parallel(128):
                            qkT[i, j] = T.exp2(qkT[i, j] * T.float32(0.18033688) - lse_shared[j])
                    T.copy(T.region(dO[bz, k * 128, bx, 0], 1, 1, 128, 1, 64), T.region(do[0, 0], 2, 128, 64))
                    T.fill(T.tvm_access_ptr(T.type_annotation("float32"), dsT.data, 0, 16384, 2), 0)
                    T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), V_shared.data, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float16"), do.data, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float32"), dsT.data, 0, 16384, 3), T.bool(False), T.bool(True), 128, 128, 64, 1, T.bool(False), 1, 0)
                    T.copy(T.region(qkT[0, 0], 1, 128, 128), T.region(qkT_cast[0, 0], 2, 128, 128))
                    T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), qkT_cast.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation("float16"), do.data, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float32"), dv.data, 0, 8192, 3), T.bool(False), T.bool(False), 128, 64, 128, 1, T.bool(False), 1, 0)
                    T.copy(T.region(Delta[bz, bx, k * 128], 1, 1, 1, 128), T.region(delta[0], 2, 128))
                    for i in T.parallel(128):
                        for j in T.parallel(128):
                            dsT_cast[i, j] = T.Cast("float16", qkT[i, j] * (dsT[i, j] - delta[j]) * T.float32(0.125))
                    T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), dsT_cast.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation("float16"), q.data, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float32"), dk.data, 0, 8192, 3), T.bool(False), T.bool(False), 128, 64, 128, 1, T.bool(False), 1, 0)
                    T.copy(T.region(dsT_cast[0, 0], 1, 128, 128), T.region(dsT_shared[0, 0], 2, 128, 128))
                    T.fill(T.tvm_access_ptr(T.type_annotation("float32"), dq.data, 0, 8192, 2), 0)
                    T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), dsT_shared.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation("float16"), K_shared, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float32"), dq.data, 0, 8192, 3), T.bool(True), T.bool(False), 128, 64, 128, 0, T.bool(False), 1, 0)
                    for i in T.parallel(128):
                        for j in T.parallel(64):
                            if k * 128 + i < 1024:
                                T.call_extern("handle", "AtomicAdd", T.address_of(dQ[bz, k * 128 + i, bx, j]), dq[i, j])
                T.copy(T.region(dv[0, 0], 1, 128, 64), T.region(dv_shared_1[0, 0], 2, 128, 64))
                T.copy(T.region(dk[0, 0], 1, 128, 64), T.region(dk_shared_1[0, 0], 2, 128, 64))
                T.copy(T.region(dv_shared_1[0, 0], 1, 128, 64), T.region(dV[bz, by * 128, bx, 0], 2, 1, 128, 1, 64))
                T.copy(T.region(dk_shared_1[0, 0], 1, 128, 64), T.region(dK[bz, by * 128, bx, 0], 2, 1, 128, 1, 64))

# Metadata omitted. Use show_meta=True in script() method to show it. ------------

5c1d8fe03971b53cf741452839bae8f924a93b25f7ebb8ca5d976f7bc7d94f1e
------------
 # from tvm.script import tir as T

@T.prim_func
def flash_fwd(Q: T.Buffer((8, 1024, 32, 64), "float16"), K: T.Buffer((8, 1024, 32, 64), "float16"), V: T.Buffer((8, 1024, 32, 64), "float16"), Output: T.Buffer((8, 1024, 32, 64), "float16"), lse: T.Buffer((8, 32, 1024), "float32")):
    # with T.block("root"):
    bx = T.launch_thread("blockIdx.x", 16)
    by = T.launch_thread("blockIdx.y", 32)
    bz = T.launch_thread("blockIdx.z", 8)
    tx = T.launch_thread("threadIdx.x", 32)
    ty = T.launch_thread("threadIdx.y", 1)
    tz = T.launch_thread("threadIdx.z", 1)
    with T.block(""):
        loop_range = T.int32()
        T.reads(Q[bz, bx * 64, by, 0], K[bz, 0:loop_range * 64 - 63, by, 0], V[bz, 0:loop_range * 64 - 63, by, 0], Output[bz, bx * 64, by, 0], lse[bz, by, bx * 64])
        T.writes()
        Q_shared = T.handle("float16", "shared.dyn")
        T.block_attr({"layout_map": {{Q_shared: metadata["tl.Layout"][0]}}})
        Q_shared_1 = T.alloc_buffer((64, 64), "float16", data=Q_shared, scope="shared.dyn")
        K_shared = T.alloc_buffer((64, 64), "float16", scope="shared.dyn")
        V_shared = T.alloc_buffer((64, 64), "float16", scope="shared.dyn")
        acc_s = T.alloc_buffer((64, 64), scope="local.fragment")
        acc_s_cast = T.alloc_buffer((64, 64), "float16", scope="local.fragment")
        acc_o = T.alloc_buffer((64, 64), scope="local.fragment")
        scores_max = T.alloc_buffer((64,), scope="local.fragment")
        scores_max_prev = T.alloc_buffer((64,), scope="local.fragment")
        scores_scale = T.alloc_buffer((64,), scope="local.fragment")
        scores_sum = T.alloc_buffer((64,), scope="local.fragment")
        logsum = T.alloc_buffer((64,), scope="local.fragment")
        T.copy(T.region(Q[bz, bx * 64, by, 0], 1, 1, 64, 1, 64), T.region(Q_shared_1[0, 0], 2, 64, 64))
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), acc_o.data, 0, 4096, 2), 0)
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), logsum.data, 0, 64, 2), 0)
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), scores_max.data, 0, 64, 2), T.float32("-inf"))
        with T.LetStmt(((bx + 1) * 64 + 64 - 1) // 64, var=loop_range):
            for k in range(loop_range):
                T.copy(T.region(K[bz, k * 64, by, 0], 1, 1, 64, 1, 64), T.region(K_shared[0, 0], 2, 64, 64))
                for i in T.parallel(64):
                    for j in T.parallel(64):
                        acc_s[i, j] = T.if_then_else(bx * 64 + i >= k * 64 + j, T.float32(0), T.float32("-inf"))
                T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), Q_shared, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float16"), K_shared.data, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float32"), acc_s.data, 0, 4096, 3), T.bool(False), T.bool(True), 64, 64, 64, 1, T.bool(False), 1, 0)
                T.copy(T.region(V[bz, k * 64, by, 0], 1, 1, 64, 1, 64), T.region(V_shared[0, 0], 2, 64, 64))
                T.copy(T.region(scores_max[0], 1, 64), T.region(scores_max_prev[0], 2, 64))
                T.reduce(T.tvm_access_ptr(T.type_annotation("float32"), acc_s.data, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float32"), scores_max.data, 0, 64, 2), "max", 1, T.bool(False))
                for i in T.parallel(64):
                    scores_scale[i] = T.exp2(scores_max_prev[i] * T.float32(0.18033688) - scores_max[i] * T.float32(0.18033688))
                for i in T.parallel(64):
                    for j in T.parallel(64):
                        acc_o[i, j] = acc_o[i, j] * scores_scale[i]
                for i in T.parallel(64):
                    for j in T.parallel(64):
                        acc_s[i, j] = T.exp2(acc_s[i, j] * T.float32(0.18033688) - scores_max[i] * T.float32(0.18033688))
                T.copy(T.region(acc_s[0, 0], 1, 64, 64), T.region(acc_s_cast[0, 0], 2, 64, 64))
                T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), acc_s_cast.data, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float16"), V_shared.data, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float32"), acc_o.data, 0, 4096, 3), T.bool(False), T.bool(False), 64, 64, 64, 1, T.bool(False), 1, 0)
                T.reduce(T.tvm_access_ptr(T.type_annotation("float32"), acc_s.data, 0, 4096, 1), T.tvm_access_ptr(T.type_annotation("float32"), scores_sum.data, 0, 64, 2), "sum", 1, T.bool(True))
                for i in T.parallel(64):
                    logsum[i] = logsum[i] * scores_scale[i] + scores_sum[i]
            for i in T.parallel(64):
                for j in T.parallel(64):
                    acc_o[i, j] = acc_o[i, j] / logsum[i]
            T.copy(T.region(acc_o[0, 0], 1, 64, 64), T.region(Output[bz, bx * 64, by, 0], 2, 1, 64, 1, 64))
            for i in T.parallel(64):
                logsum[i] = T.log2(logsum[i]) + scores_max[i] * T.float32(0.18033688)
            T.copy(T.region(logsum[0], 1, 64), T.region(lse[bz, by, bx * 64], 2, 1, 1, 64))

# Metadata omitted. Use show_meta=True in script() method to show it. ------------

8ab84ec9e25d51969a97fe19ee29b8ce02e88f4ef825b41c8ced14728d913f8d
------------
 # from tvm.script import tir as T

@T.prim_func
def flash_bwd_prep(O: T.Buffer((8, 1024, 32, 64), "float16"), dO: T.Buffer((8, 1024, 32, 64), "float16"), Delta: T.Buffer((8, 32, 1024), "float32")):
    # with T.block("root"):
    bx = T.launch_thread("blockIdx.x", 32)
    by = T.launch_thread("blockIdx.y", 32)
    bz = T.launch_thread("blockIdx.z", 8)
    tx = T.launch_thread("threadIdx.x", 128)
    ty = T.launch_thread("threadIdx.y", 1)
    tz = T.launch_thread("threadIdx.z", 1)
    with T.block(""):
        T.reads(O[bz, by * 32, bx, 0:33], dO[bz, by * 32, bx, 0:33], Delta[bz, bx, by * 32])
        T.writes()
        o = T.alloc_buffer((32, 32), "float16", scope="local.fragment")
        do = T.alloc_buffer((32, 32), "float16", scope="local.fragment")
        acc = T.alloc_buffer((32, 32), scope="local.fragment")
        delta = T.alloc_buffer((32,), scope="local.fragment")
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), acc.data, 0, 1024, 2), 0)
        for k in range(2):
            T.copy(T.region(O[bz, by * 32, bx, k * 32], 1, 1, 32, 1, 32), T.region(o[0, 0], 2, 32, 32))
            T.copy(T.region(dO[bz, by * 32, bx, k * 32], 1, 1, 32, 1, 32), T.region(do[0, 0], 2, 32, 32))
            for i in T.parallel(32):
                for j in T.parallel(32):
                    acc[i, j] = acc[i, j] + T.Cast("float32", o[i, j] * do[i, j])
        T.reduce(T.tvm_access_ptr(T.type_annotation("float32"), acc.data, 0, 1024, 1), T.tvm_access_ptr(T.type_annotation("float32"), delta.data, 0, 32, 2), "sum", 1, T.bool(True))
        T.copy(T.region(delta[0], 1, 32), T.region(Delta[bz, bx, by * 32], 2, 1, 1, 32)) ------------

a403fb01108aaa52a46b8473024e9d69682958197e37d8facf5cfc3fb004bdd1
------------
 # from tvm.script import tir as T

@T.prim_func
def flash_bwd_post(dQ: T.Buffer((8, 1024, 32, 64), "float32"), dQ_out: T.Buffer((8, 1024, 32, 64), "float16")):
    # with T.block("root"):
    bx = T.launch_thread("blockIdx.x", 16)
    by = T.launch_thread("blockIdx.y", 32)
    bz = T.launch_thread("blockIdx.z", 8)
    tx = T.launch_thread("threadIdx.x", 128)
    ty = T.launch_thread("threadIdx.y", 1)
    tz = T.launch_thread("threadIdx.z", 1)
    with T.block(""):
        T.reads(dQ[bz, bx * 64, by, 0], dQ_out[bz, bx * 64, by, 0])
        T.writes()
        T.block_attr({"layout_map": {{dQ.data: metadata["tl.Layout"][0]}}})
        T.copy(T.region(dQ[bz, bx * 64, by, 0], 1, 1, 64, 1, 64), T.region(dQ_out[bz, bx * 64, by, 0], 2, 1, 64, 1, 64))

# Metadata omitted. Use show_meta=True in script() method to show it. ------------

c5d1256213ca0e50867cb789d1bb40ca307e29cff04358959f772528ca7ac61b
------------
 # from tvm.script import tir as T

@T.prim_func
def flash_bwd(Q: T.Buffer((8, 1024, 32, 64), "float16"), K: T.Buffer((8, 1024, 32, 64), "float16"), V: T.Buffer((8, 1024, 32, 64), "float16"), dO: T.Buffer((8, 1024, 32, 64), "float16"), lse: T.Buffer((8, 32, 1024), "float32"), Delta: T.Buffer((8, 32, 1024), "float32"), dQ: T.Buffer((8, 1024, 32, 64), "float32"), dK: T.Buffer((8, 1024, 32, 64), "float16"), dV: T.Buffer((8, 1024, 32, 64), "float16")):
    # with T.block("root"):
    bx = T.launch_thread("blockIdx.x", 32)
    by = T.launch_thread("blockIdx.y", 8)
    bz = T.launch_thread("blockIdx.z", 8)
    tx = T.launch_thread("threadIdx.x", 32)
    ty = T.launch_thread("threadIdx.y", 1)
    tz = T.launch_thread("threadIdx.z", 1)
    with T.block(""):
        loop_st = T.int32()
        loop_ed = T.int32()
        T.reads(K[bz, by * 128, bx, 0], V[bz, by * 128, bx, 0], Q[bz, loop_st * 128:loop_st * 128 + (loop_ed * 128 - loop_st * 128 - 127), bx, 0], lse[bz, bx, loop_st * 128:loop_st * 128 + (loop_ed * 128 - loop_st * 128 - 127)], dO[bz, loop_st * 128:loop_st * 128 + (loop_ed * 128 - loop_st * 128 - 127), bx, 0], Delta[bz, bx, loop_st * 128:loop_st * 128 + (loop_ed * 128 - loop_st * 128 - 127)], dQ[bz, loop_st * 128:loop_st * 128 + (loop_ed * 128 - loop_st * 128), bx, 0:64], dV[bz, by * 128, bx, 0], dK[bz, by * 128, bx, 0])
        T.writes()
        K_shared = T.handle("float16", "shared.dyn")
        dk_shared = T.handle("float16", "shared.dyn")
        dv_shared = T.handle("float16", "shared.dyn")
        T.block_attr({"layout_map": {{K_shared: metadata["tl.Layout"][0], dQ.data: metadata["tl.Layout"][1], dk_shared: metadata["tl.Layout"][2], dv_shared: metadata["tl.Layout"][3]}}})
        K_shared_1 = T.alloc_buffer((128, 64), "float16", data=K_shared, scope="shared.dyn")
        dsT_shared = T.alloc_buffer((128, 128), "float16", scope="shared.dyn")
        q = T.alloc_buffer((128, 64), "float16", scope="shared.dyn")
        V_shared = T.alloc_buffer((128, 64), "float16", scope="shared.dyn")
        qkT = T.alloc_buffer((128, 128), scope="local.fragment")
        dsT = T.alloc_buffer((128, 128), scope="local.fragment")
        qkT_cast = T.alloc_buffer((128, 128), "float16", scope="local.fragment")
        dsT_cast = T.alloc_buffer((128, 128), "float16", scope="local.fragment")
        lse_shared = T.alloc_buffer((128,), scope="shared.dyn")
        delta = T.alloc_buffer((128,), scope="shared.dyn")
        do = T.alloc_buffer((128, 64), "float16", scope="shared.dyn")
        dv = T.alloc_buffer((128, 64), scope="local.fragment")
        dk = T.alloc_buffer((128, 64), scope="local.fragment")
        dq = T.alloc_buffer((128, 64), scope="local.fragment")
        dv_shared_1 = T.alloc_buffer((128, 64), "float16", data=dv_shared, scope="shared.dyn")
        dk_shared_1 = T.alloc_buffer((128, 64), "float16", data=dk_shared, scope="shared.dyn")
        T.copy(T.region(K[bz, by * 128, bx, 0], 1, 1, 128, 1, 64), T.region(K_shared_1[0, 0], 2, 128, 64))
        T.copy(T.region(V[bz, by * 128, bx, 0], 1, 1, 128, 1, 64), T.region(V_shared[0, 0], 2, 128, 64))
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), dv.data, 0, 8192, 2), 0)
        T.fill(T.tvm_access_ptr(T.type_annotation("float32"), dk.data, 0, 8192, 2), 0)
        with T.LetStmt(by * 128 // 128, var=loop_st):
            with T.LetStmt(8, var=loop_ed):
                for k in range(loop_st, loop_st + (loop_ed - loop_st)):
                    T.copy(T.region(Q[bz, k * 128, bx, 0], 1, 1, 128, 1, 64), T.region(q[0, 0], 2, 128, 64))
                    T.fill(T.tvm_access_ptr(T.type_annotation("float32"), qkT.data, 0, 16384, 2), 0)
                    T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), K_shared, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float16"), q.data, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float32"), qkT.data, 0, 16384, 3), T.bool(False), T.bool(True), 128, 128, 64, 1, T.bool(False), 1, 0)
                    T.copy(T.region(lse[bz, bx, k * 128], 1, 1, 1, 128), T.region(lse_shared[0], 2, 128))
                    for i in T.parallel(128):
                        for j in T.parallel(128):
                            qkT[i, j] = T.exp2(qkT[i, j] * T.float32(0.18033688) - lse_shared[j])
                    for i in T.parallel(128):
                        for j in T.parallel(128):
                            qkT[i, j] = T.if_then_else(by * 128 + i <= k * 128 + j, qkT[i, j], T.float32(0))
                    T.copy(T.region(dO[bz, k * 128, bx, 0], 1, 1, 128, 1, 64), T.region(do[0, 0], 2, 128, 64))
                    T.fill(T.tvm_access_ptr(T.type_annotation("float32"), dsT.data, 0, 16384, 2), 0)
                    T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), V_shared.data, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float16"), do.data, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float32"), dsT.data, 0, 16384, 3), T.bool(False), T.bool(True), 128, 128, 64, 1, T.bool(False), 1, 0)
                    T.copy(T.region(qkT[0, 0], 1, 128, 128), T.region(qkT_cast[0, 0], 2, 128, 128))
                    T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), qkT_cast.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation("float16"), do.data, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float32"), dv.data, 0, 8192, 3), T.bool(False), T.bool(False), 128, 64, 128, 1, T.bool(False), 1, 0)
                    T.copy(T.region(Delta[bz, bx, k * 128], 1, 1, 1, 128), T.region(delta[0], 2, 128))
                    for i in T.parallel(128):
                        for j in T.parallel(128):
                            dsT_cast[i, j] = T.Cast("float16", qkT[i, j] * (dsT[i, j] - delta[j]) * T.float32(0.125))
                    T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), dsT_cast.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation("float16"), q.data, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float32"), dk.data, 0, 8192, 3), T.bool(False), T.bool(False), 128, 64, 128, 1, T.bool(False), 1, 0)
                    T.copy(T.region(dsT_cast[0, 0], 1, 128, 128), T.region(dsT_shared[0, 0], 2, 128, 128))
                    T.fill(T.tvm_access_ptr(T.type_annotation("float32"), dq.data, 0, 8192, 2), 0)
                    T.gemm(T.tvm_access_ptr(T.type_annotation("float16"), dsT_shared.data, 0, 16384, 1), T.tvm_access_ptr(T.type_annotation("float16"), K_shared, 0, 8192, 1), T.tvm_access_ptr(T.type_annotation("float32"), dq.data, 0, 8192, 3), T.bool(True), T.bool(False), 128, 64, 128, 0, T.bool(False), 1, 0)
                    for i in T.parallel(128):
                        for j in T.parallel(64):
                            if k * 128 + i < 1024:
                                T.call_extern("handle", "AtomicAdd", T.address_of(dQ[bz, k * 128 + i, bx, j]), dq[i, j])
                T.copy(T.region(dv[0, 0], 1, 128, 64), T.region(dv_shared_1[0, 0], 2, 128, 64))
                T.copy(T.region(dk[0, 0], 1, 128, 64), T.region(dk_shared_1[0, 0], 2, 128, 64))
                T.copy(T.region(dv_shared_1[0, 0], 1, 128, 64), T.region(dV[bz, by * 128, bx, 0], 2, 1, 128, 1, 64))
                T.copy(T.region(dk_shared_1[0, 0], 1, 128, 64), T.region(dK[bz, by * 128, bx, 0], 2, 1, 128, 1, 64))

# Metadata omitted. Use show_meta=True in script() method to show it. ------------

2115cc9b0c9bf3f9be3a22fa0f9677b780bedc7ca0324fb487f860c49aeb1118
.

=============================== warnings summary ===============================
../3rdparty/tvm/python/tvm/relay/op/contrib/ethosn.py:20
  /workspace/tilelang/tilelang/../3rdparty/tvm/python/tvm/relay/op/contrib/ethosn.py:20: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
    from distutils.version import LooseVersion

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
========================= 1 passed, 1 warning in 3.61s =========================
